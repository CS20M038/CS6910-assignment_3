{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_3_Arjun_Version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOznNU2mJUMBWERsum+geFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "507439c4aab6494daa39f06da953f958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_05064d08375b4bbeb2f9bc8260230d3c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c34756a86a34bb08ea1c7704cf1e20d",
              "IPY_MODEL_2c0bad4dd0e44616aafb9bf239c0addc"
            ]
          }
        },
        "05064d08375b4bbeb2f9bc8260230d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c34756a86a34bb08ea1c7704cf1e20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_c6c0ca37474d48749d1f4893ae8d4fac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.24MB of 1.24MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34d505297a964e4e894956b868209a42"
          }
        },
        "2c0bad4dd0e44616aafb9bf239c0addc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b0b1ea3a2ef54bf18a9805f202c01666",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55fdc32faece4dfcbdced312cdb81080"
          }
        },
        "c6c0ca37474d48749d1f4893ae8d4fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34d505297a964e4e894956b868209a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0b1ea3a2ef54bf18a9805f202c01666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55fdc32faece4dfcbdced312cdb81080": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CS20M038/CS6910-assignment_3/blob/main/DL_Assignment_3_Arjun_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZZ6KEDllSsM"
      },
      "source": [
        "#Setting Up the Notebook\n",
        "This section involves the importing of important librabries,setting wandb,checking for GPU and mounting onto the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvmZ1iU5yfHl"
      },
      "source": [
        "#import all the libraries that would be used in this notebook\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.layers import LSTM,Dense\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "V6yiEpEyimkY",
        "outputId": "ec945b6e-227a-45dd-f0f8-7672849ff80b"
      },
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 56.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 43.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQsnGeZKfcY0",
        "outputId": "2063c249-cdb3-4fd2-9cae-38457e70bb48"
      },
      "source": [
        "# to mount to the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYk1NQ0rArPL",
        "outputId": "656f81c7-c1ba-4dec-f311-02b0cc995ab7"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHiZk1MjTkK"
      },
      "source": [
        "# Loading Data and Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXE2OtCp8e_4"
      },
      "source": [
        "#make sure to upload the Dakshina Dataset in your drive and name the folder as Dakshina_Dataset\n",
        "dakshina_dataset_hindi = '/content/drive/MyDrive/Dakshina_Dataset/hi/lexicons'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T46N4HDdid3g"
      },
      "source": [
        "#all the dataset paths\n",
        "dev_dataset_path = os.path.join(dakshina_dataset_hindi,\"hi.translit.sampled.dev.tsv\")\n",
        "train_dataset_path = os.path.join(dakshina_dataset_hindi,\"hi.translit.sampled.train.tsv\")\n",
        "test_dataset_path = os.path.join(dakshina_dataset_hindi,\"hi.translit.sampled.test.tsv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGPG6ZxH9nlE"
      },
      "source": [
        "#function to load the dataset from the dataset path\n",
        "def load_dataset(path,test_dataset = False):\n",
        "    dataset = open(path)\n",
        "    read_dataset = csv.reader(dataset,delimiter = '\\t')\n",
        "    x = []\n",
        "    y = []\n",
        "    for row in read_dataset:\n",
        "        x.append(row[1])\n",
        "        if test_dataset:\n",
        "            y.append(row[0])\n",
        "        else:\n",
        "            y.append(\"\\t\"+row[0]+'\\n')\n",
        "\n",
        "    return np.array(x),np.array(y)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_ZVvh2rjPT1"
      },
      "source": [
        "x_raw_train,y_raw_train = load_dataset(train_dataset_path,False)\n",
        "x_raw_val,y_raw_val = load_dataset(dev_dataset_path,True)\n",
        "x_raw_test,y_raw_test = load_dataset(test_dataset_path,True)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVR6TsUuj9Wl"
      },
      "source": [
        "english_alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
        "english_alpha2index = {\"PAD\": 0}\n",
        "for index,alpha in enumerate(english_alphabets):\n",
        "    english_alpha2index[alpha] = index + 1\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabets.append('\\t')\n",
        "hindi_alphabets.append('\\n')\n",
        "\n",
        "hindi_alpha2index = {\"PAD\" : 0}\n",
        "for index,alpha in enumerate(hindi_alphabets):\n",
        "    hindi_alpha2index[alpha] = index + 1\n",
        "\n",
        "english_index2alpha = {0: \"PAD\"}\n",
        "hindi_index2alpha = {0: \"PAD\"}\n",
        "\n",
        "\n",
        "for index,alpha in enumerate(english_alphabets):\n",
        "    english_index2alpha[index + 1] = alpha\n",
        "\n",
        "for index,alpha in enumerate(hindi_alphabets):\n",
        "    hindi_index2alpha[index + 1] = alpha\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtSXANSvQRE1"
      },
      "source": [
        "def get_integer_encode(word,alpha2index,max_length = 25):\n",
        "    integer_encode = np.zeros((max_length,),dtype='int')\n",
        "    for index,alpha in enumerate(word):\n",
        "        integer_encode[index] = alpha2index[alpha]\n",
        "    return integer_encode\n",
        "\n",
        "def get_word_decode(integer_encoded,index2alpha):\n",
        "    \n",
        "    word = \"\"\n",
        "    for integer in integer_encoded:\n",
        "        word += index2alpha[integer]\n",
        "    return word\n",
        "\n",
        "\n",
        "def encode_docs(docs,alpha2index,max_length = 25):\n",
        "    encoded_docs = np.zeros((docs.shape[0],max_length),dtype='int')\n",
        "    for index,word in enumerate(docs):\n",
        "        encoded_docs[index] = get_integer_encode(word,alpha2index)\n",
        "    return encoded_docs\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-RARFzyS_lY"
      },
      "source": [
        "integer_encoded_x_train = encode_docs(x_raw_train,english_alpha2index)\n",
        "integer_encoded_y_train = encode_docs(y_raw_train,hindi_alpha2index)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpCierShU1Tl"
      },
      "source": [
        "decoder_input_data = np.zeros((integer_encoded_y_train.shape[0],25,len(hindi_alpha2index)),dtype=\"float32\")\n",
        "decoder_output_data = np.zeros((integer_encoded_y_train.shape[0],25,len(hindi_alpha2index)),dtype=\"float32\")\n",
        "\n",
        "for i,integer_encoded_data in enumerate(integer_encoded_y_train):\n",
        "    for t,integer in enumerate(integer_encoded_data):\n",
        "        decoder_input_data[i, t, integer]= 1.0\n",
        "        if t > 0:\n",
        "            decoder_output_data[i, t - 1, integer] = 1.0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbKvHQ_cTDuA"
      },
      "source": [
        "integer_encoded_x_val = encode_docs(x_raw_val,english_alpha2index)\n",
        "integer_encoded_y_val = encode_docs(y_raw_val,hindi_alpha2index)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpFAYu3TMHs"
      },
      "source": [
        "decoder_input_val = np.zeros((integer_encoded_y_val.shape[0],25,len(hindi_alpha2index)),dtype=\"float32\")\n",
        "decoder_output_val = np.zeros((integer_encoded_y_val.shape[0],25,len(hindi_alpha2index)),dtype=\"float32\")\n",
        "\n",
        "for i,integer_encoded_data in enumerate(integer_encoded_y_val):\n",
        "    for t,integer in enumerate(integer_encoded_data):\n",
        "        decoder_input_val[i, t, integer]= 1.0\n",
        "        if t > 0:\n",
        "            decoder_output_val[i, t - 1, integer] = 1.0"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlpmNQYHkw-f"
      },
      "source": [
        "# The Generic RNN Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR6YC0PRJTFz"
      },
      "source": [
        "class Customcallback(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "    def on_epoch_end(self,epoch,logs = None):\n",
        "        print(\"in epoch\",epoch)\n",
        "        print(logs)\n",
        "\n",
        "        "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Yo5nQPopVL"
      },
      "source": [
        "class RNN():\n",
        "\n",
        "    def __init__(self,embedding_size,num_encoder_layers,num_decoder_layers,hidden_layer_size,cell_type,drop_out_ratio,in_char_size,out_char_size,input_len = 25):\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.cell_type = cell_type\n",
        "        self.drop_out_ratio = drop_out_ratio\n",
        "        self.in_char_size = in_char_size\n",
        "        self.out_char_size = out_char_size\n",
        "        self.input_len = input_len\n",
        "\n",
        "        self._build_rnn_network()\n",
        "\n",
        "    def _build_rnn_network(self):\n",
        "\n",
        "        # Embedding \n",
        "        embedding_inputs = keras.Input(shape=(None,))\n",
        "        embedding_layer = Embedding(self.in_char_size, self.embedding_size , input_length=self.input_len)\n",
        "        encoder_inputs = embedding_layer(embedding_inputs)\n",
        "\n",
        "        #Encoder\n",
        "\n",
        "        self.encoder_layers = []\n",
        "        for _ in range(self.num_encoder_layers-1):\n",
        "            encoder_layer = getattr(layers,self.cell_type)(self.hidden_layer_size, dropout=self.drop_out_ratio, return_sequences=True)\n",
        "            encoder_inputs = encoder_layer(encoder_inputs)\n",
        "            self.encoder_layers.append(encoder_layer)\n",
        "\n",
        "        last_encoder_layer = getattr(layers,self.cell_type)(self.hidden_layer_size, dropout=self.drop_out_ratio, return_state=True)\n",
        "        encoder_outputs,*encoder_states = last_encoder_layer(encoder_inputs)\n",
        "        self.encoder_layers.append(last_encoder_layer)\n",
        "\n",
        "        #Decoder\n",
        "        initial_decoder_inputs = keras.Input(shape=(None, self.out_char_size))\n",
        "        decoder_inputs = initial_decoder_inputs\n",
        "\n",
        "        self.decoder_layers = []\n",
        "\n",
        "        for _ in range(self.num_decoder_layers):\n",
        "\n",
        "            decoder_layer = getattr(layers,self.cell_type)(self.hidden_layer_size, dropout=self.drop_out_ratio, return_sequences=True,return_state=True)\n",
        "            decoder_inputs,*decoder_states = decoder_layer(decoder_inputs,initial_state=encoder_states)\n",
        "            self.decoder_layers.append(decoder_layer)\n",
        "\n",
        "        decoder_outputs = decoder_inputs\n",
        "        decoder_dense = Dense(self.out_char_size, activation=\"softmax\")\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        model = keras.Model([embedding_inputs, initial_decoder_inputs], decoder_outputs) \n",
        "\n",
        "        self.model = model\n",
        "        self.embedding_inputs = embedding_inputs\n",
        "        self.encoder_states = encoder_states\n",
        "        self.decoder_inputs = initial_decoder_inputs\n",
        "        self.decoder_dense = decoder_dense\n",
        "\n",
        "    def compile(self,optimizer=\"rmsprop\"):\n",
        "\n",
        "        self.model.compile(\n",
        "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "        )\n",
        "\n",
        "    def fit(self,encoder_input,decoder_input,decoder_output,batch_size = 64,epochs = 5):\n",
        "        self.model.fit(\n",
        "        [encoder_input, decoder_input],\n",
        "        decoder_output,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks = [wandb.keras.WandbCallback()],\n",
        "        validation_split=0.1\n",
        "        )\n",
        "\n",
        "    def build_inference_model(self):\n",
        "\n",
        "        self.encoder_model = Model(self.embedding_inputs,self.encoder_states)\n",
        "\n",
        "        decoder_state_input = []\n",
        "        for i in range(len(self.encoder_states)) :\n",
        "            new_state = keras.Input(shape=(self.hidden_layer_size,))\n",
        "            decoder_state_input.append(new_state)\n",
        "\n",
        "        initial_decoder_inputs = self.decoder_inputs\n",
        "\n",
        "        decoder_inputs = initial_decoder_inputs\n",
        "        for layer in self.decoder_layers :\n",
        "            decoder_inputs, *decoder_states = layer(decoder_inputs,initial_state=decoder_state_input)\n",
        "\n",
        "        decoder_outputs = decoder_inputs\n",
        "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
        "\n",
        "        self.decoder_model = Model(\n",
        "            [initial_decoder_inputs] + decoder_state_input,\n",
        "            [decoder_outputs] + decoder_states\n",
        "            )\n",
        "\n",
        "\n",
        "    def _decode_sequence(self,word):\n",
        "\n",
        "        input_seq = get_integer_encode(word,english_alpha2index)\n",
        "        input_seq = input_seq.reshape(1,25)\n",
        "    \n",
        "        states_value = self.encoder_model.predict(input_seq)\n",
        "\n",
        "        target_seq = np.zeros((1,1, len(hindi_alpha2index)))\n",
        "        target_seq[0,0, hindi_alpha2index[\"\\t\"]] = 1.0\n",
        "\n",
        "        decoded_word = \"\"\n",
        "        while True:\n",
        "            output_tokens, *states = self.decoder_model.predict([target_seq] + [states_value])\n",
        "\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "            sampled_char = hindi_index2alpha[sampled_token_index]\n",
        "        \n",
        "            if sampled_char == \"\\n\" or len(decoded_word) > 25:\n",
        "                break\n",
        "\n",
        "            decoded_word += sampled_char\n",
        "\n",
        "            target_seq = np.zeros((1, 1,len(hindi_alpha2index)))\n",
        "            target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "            states_value = [states[i] for i in range(len(states))]\n",
        "\n",
        "        return decoded_word\n",
        "\n",
        "    def evaluate(self,X_test,Y_test):\n",
        "        correct = 0\n",
        "        for english_word,hindi_word in zip(X_test,Y_test):\n",
        "            predicted_hindi_word = self._decode_sequence(english_word)\n",
        "            if predicted_hindi_word == hindi_word:\n",
        "                correct += 1\n",
        "\n",
        "        acc = (correct/X_test.shape[0])*100\n",
        "        return acc\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egjoGcjrbsD"
      },
      "source": [
        "hyperparameters = {\n",
        "    \"embedding_size\" : 125,\n",
        "    \"num_encoder_layers\" : 1,\n",
        "    \"num_decoder_layers\" : 1,\n",
        "    \"hidden_layer_size\" : 256,\n",
        "    \"cell_type\" : \"LSTM\",\n",
        "    \"drop_out_ratio\": 0.2,\n",
        "    \"in_char_size\": len(english_alpha2index),\n",
        "    \"out_char_size\": len(hindi_alpha2index),\n",
        "    \"input_len\": 25,\n",
        "    }\n",
        "\n",
        "net = RNN(**hyperparameters)\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34ERBPOM-Ygu",
        "outputId": "91b42cd5-5f66-4099-a552-ea918b02df20"
      },
      "source": [
        "net.compile()\n",
        "net.fit(integer_encoded_x_train, decoder_input_data,decoder_output_data,epochs = 25)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "691/691 [==============================] - 11s 12ms/step - loss: 1.1295 - accuracy: 0.7370 - val_loss: 0.7722 - val_accuracy: 0.7994\n",
            "Epoch 2/25\n",
            "691/691 [==============================] - 7s 11ms/step - loss: 0.7803 - accuracy: 0.7937 - val_loss: 0.6081 - val_accuracy: 0.8323\n",
            "Epoch 3/25\n",
            "691/691 [==============================] - 7s 11ms/step - loss: 0.6390 - accuracy: 0.8254 - val_loss: 0.4880 - val_accuracy: 0.8647\n",
            "Epoch 4/25\n",
            "691/691 [==============================] - 7s 11ms/step - loss: 0.5085 - accuracy: 0.8580 - val_loss: 0.3742 - val_accuracy: 0.8920\n",
            "Epoch 5/25\n",
            "691/691 [==============================] - 7s 11ms/step - loss: 0.4062 - accuracy: 0.8827 - val_loss: 0.3133 - val_accuracy: 0.9123\n",
            "Epoch 6/25\n",
            "691/691 [==============================] - 7s 11ms/step - loss: 0.3325 - accuracy: 0.9019 - val_loss: 0.2567 - val_accuracy: 0.9251\n",
            "Epoch 7/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.2808 - accuracy: 0.9162 - val_loss: 0.2282 - val_accuracy: 0.9321\n",
            "Epoch 8/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.2431 - accuracy: 0.9267 - val_loss: 0.2074 - val_accuracy: 0.9384\n",
            "Epoch 9/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.2168 - accuracy: 0.9343 - val_loss: 0.1948 - val_accuracy: 0.9406\n",
            "Epoch 10/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1964 - accuracy: 0.9398 - val_loss: 0.1840 - val_accuracy: 0.9443\n",
            "Epoch 11/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1786 - accuracy: 0.9448 - val_loss: 0.1807 - val_accuracy: 0.9453\n",
            "Epoch 12/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1661 - accuracy: 0.9487 - val_loss: 0.1723 - val_accuracy: 0.9476\n",
            "Epoch 13/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1527 - accuracy: 0.9528 - val_loss: 0.1710 - val_accuracy: 0.9483\n",
            "Epoch 14/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1440 - accuracy: 0.9556 - val_loss: 0.1726 - val_accuracy: 0.9488\n",
            "Epoch 15/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1382 - accuracy: 0.9581 - val_loss: 0.1723 - val_accuracy: 0.9501\n",
            "Epoch 16/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1346 - accuracy: 0.9606 - val_loss: 0.1802 - val_accuracy: 0.9498\n",
            "Epoch 17/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1331 - accuracy: 0.9627 - val_loss: 0.1760 - val_accuracy: 0.9506\n",
            "Epoch 18/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1304 - accuracy: 0.9643 - val_loss: 0.1844 - val_accuracy: 0.9498\n",
            "Epoch 19/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1264 - accuracy: 0.9659 - val_loss: 0.1919 - val_accuracy: 0.9499\n",
            "Epoch 20/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1204 - accuracy: 0.9677 - val_loss: 0.1871 - val_accuracy: 0.9503\n",
            "Epoch 21/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1159 - accuracy: 0.9690 - val_loss: 0.1871 - val_accuracy: 0.9513\n",
            "Epoch 22/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1117 - accuracy: 0.9704 - val_loss: 0.1882 - val_accuracy: 0.9501\n",
            "Epoch 23/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1075 - accuracy: 0.9719 - val_loss: 0.1915 - val_accuracy: 0.9503\n",
            "Epoch 24/25\n",
            "691/691 [==============================] - 7s 10ms/step - loss: 0.1034 - accuracy: 0.9729 - val_loss: 0.1946 - val_accuracy: 0.9502\n",
            "Epoch 25/25\n",
            "691/691 [==============================] - 7s 11ms/step - loss: 0.0992 - accuracy: 0.9744 - val_loss: 0.1989 - val_accuracy: 0.9507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxCoWAj-DMtg"
      },
      "source": [
        "net.build_inference_model()"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSpbCokI093K",
        "outputId": "b7209574-bae1-4f2d-b473-abe322923acb"
      },
      "source": [
        "net.encoder_model.summary()\n",
        "combined_validation_data = list(zip(x_raw_val,y_raw_val))\n",
        "np.random.shuffle(combined_validation_data)\n",
        "(x_val,y_val) = zip(*combined_validation_data)\n",
        "x_val_500,y_val_500 = np.array(x_val[:500]),np.array(y_val[:500])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_35 (InputLayer)        [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_10 (Embedding)     (None, None, 125)         3375      \n",
            "_________________________________________________________________\n",
            "lstm_40 (LSTM)               [(None, 256), (None, 256) 391168    \n",
            "=================================================================\n",
            "Total params: 394,543\n",
            "Trainable params: 394,543\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSKrqPNNDWgP",
        "outputId": "187073cf-caaa-4ee8-c5c5-899d27b89a24"
      },
      "source": [
        "net.evaluate(x_val_500,y_val_500)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patila  पतीला  पतिला\n",
            "nakum  नकुम  नाकुम\n",
            "punaravalokan  पुनरावलोकन  पुनरावलों\n",
            "rajkaran  राजकरण  राजकरण\n",
            "hitake  हाईटेक  हितके\n",
            "shyamdas  श्यामदास  श्यामद्स\n",
            "rasna  रसना  रसन्ना\n",
            "celina  सेलीना  सेलिना\n",
            "cricketeron  क्रिकेटरों  क्रिक्टरों\n",
            "orlando  ओरलैंडो  ऑर्लेंडो\n",
            "gangabai  गंगाबाई  गंगाबाई\n",
            "thanks  थैंक्स  ठंक्स\n",
            "najar  नजर  नज़र\n",
            "duptta  दुपट्टा  दुप्ता\n",
            "darvaajaa  दरवाजा  दरवाजा\n",
            "typhoon  टाइफून  तिप्ण\n",
            "marte  मारते  मारते\n",
            "interpol  इंटरपोल  इंटरपोल\n",
            "chhedte  छेड़ते  छेड़ते\n",
            "sampradayikata  सांप्रदायिकता  संप्रदायिकता\n",
            "hansda  हंसदा  हंसदा\n",
            "kavval  कव्वाल  काववाल\n",
            "mansikta  मानसिकता  मानीसिकता\n",
            "dhanarthi  धनार्थी  धनरती\n",
            "riyon  रियान  रियों\n",
            "kanton  कांटों  कंतों\n",
            "gomatinagar  गोमतीनगर  गोमेटनगर\n",
            "startup  स्टार्टअप  स्टार्टप\n",
            "bali  बली  बली\n",
            "jockey  जॉकी  जॉकी\n",
            "sampradayik  सांप्रदायिक  संप्रदायिक\n",
            "ncg  एससीजी  एनएीजी\n",
            "chitthiyon  चिठ्ठियों  चित्तियों\n",
            "dhoondhte  ढूंढते  ढूंडते\n",
            "profesion  प्रोफेशन  प्रोफ़ीशन\n",
            "vipreet  विपरीत  विप्रेत\n",
            "indy  इंडी  इंडी\n",
            "baikunth  बैकुंठ  बेनुंता\n",
            "deghat  देघाट  देघात\n",
            "judge  जज  जुड़े\n",
            "rehayashi  रिहायशी  रेहाशी\n",
            "aasheervachan  आशीर्वचन  आशिर्षण\n",
            "anadarana  अनादरण  अनाराना\n",
            "aparamprik  अपारंपरिक  अपरंस्विक\n",
            "yakarman  एकरमैन  यकार्मण\n",
            "pavel  पॉवेल  पेवल\n",
            "dilayi  दिलायी  दिलाई\n",
            "pachranga  पचरंगा  पछारंगा\n",
            "vishleshanon  विश्लेषणों  विश्लेषों\n",
            "tapman  तापमान  ताममान\n",
            "agrata  अग्रता  अग्रता\n",
            "chhidi  छिड़ी  छिड़ी\n",
            "dimagi  दिमागी  दीमागी\n",
            "sodon  सौदों  सोदों\n",
            "chaunkti  चौंकती  चौंकती\n",
            "jivantata  जीवंतता  जीवंता\n",
            "motipur  मोतीपुर  मोतिपुर\n",
            "daalti  डालती  दालती\n",
            "saptarshi  सप्तर्षि  सप्तराशी\n",
            "ruturaj  ऋतुराज  रुतुराज\n",
            "raghukul  रघुकुल  रघुकुल\n",
            "sumantra  सुमंत्र  सुमानत्र\n",
            "chhidaa  छिड़ा  छिड़ा\n",
            "pehlavi  पहलवी  पहलवाई\n",
            "janjevan  जनजीवन  जानजेवन\n",
            "pichhad  पिछड़  पिछड़\n",
            "dharma  धर्मा  धर्मा\n",
            "anjam  अंजाम  अंजम\n",
            "praachinatam  प्राचीनतम  प्राचीनता\n",
            "badhaani  बढ़ानी  बढ़ानी\n",
            "bager  बग़ैर  बागेर\n",
            "laybaddh  लयबद्ध  लायबद्ध\n",
            "upadeshakon  उपदेशकों  उपदेशकों\n",
            "mangwana  मंगवाना  मंगवाना\n",
            "majedar  मजेदार  मजेदार\n",
            "sharir  शरीर  शरीर\n",
            "yogaphal  योगफल  योगपाल\n",
            "kalis  कालिस  कालिस\n",
            "andhera  अंधेरा  अंधरा\n",
            "sidan  सीडान  सिदान\n",
            "abhinatri  अभिनत्री  अभिनत्री\n",
            "age  ऐज  आगे\n",
            "pixel  पिक्सेल  पिक्सल\n",
            "voice  वॉयस  वॉइस\n",
            "upmanyu  उपमन्यु  उपमायु\n",
            "chhach  छाछ  छा\n",
            "leader  लीडर  लेडर\n",
            "dhut  धुत  धूत\n",
            "julana  जुलना  जुलना\n",
            "dhakna  ढकना  ढकना\n",
            "sudan  सूडान  सुदन\n",
            "avastweak  अवास्तविक  अवस्तवापी\n",
            "rasanaa  रसना  रसना\n",
            "tapasvini  तपस्विनी  तपस्विनी\n",
            "semi  सेमी  सेमी\n",
            "akaash  अकाश  अकाश\n",
            "niveshit  निवेशित  निवेशित\n",
            "hold  होल्ड  होल्ड\n",
            "phte  फटे  फीटे\n",
            "galatfahmiyon  गलतफहमियों  गलतद्पामियों\n",
            "pratispardhaein  प्रतिस्पर्धाएं  प्रतिस्पार्याओं\n",
            "ozal  ओझल  ओजल\n",
            "manonyan  मनोनयन  मनोनियां\n",
            "labh  लभ  लाभ\n",
            "jajapur  जाजपुर  जजपूर\n",
            "urwarkata  उर्वरकता  उर्करकाता\n",
            "mahaul  माहौल  महौल\n",
            "bataunga  बताऊंगा  बटौंगा\n",
            "jaiswal  जयसवाल  जैससाल\n",
            "bandhu  बंधु  बंधु\n",
            "terence  टेरेंस  टेरेंस\n",
            "chare  चारे  चरे\n",
            "mandarin  मैंडरिन  मांडरिन\n",
            "kamariya  कमरिया  कमारिया\n",
            "attention  अटेंशन  अटेंटन\n",
            "rakhoge  रखोगे  रखोंगे\n",
            "arendi  अरंडी  एरेंडी\n",
            "draupadi  द्रोपदी  द्रौपदी\n",
            "muchlka  मुचलका  मुच्लका\n",
            "nijaampur  निजामपुर  निजामपुर\n",
            "karnafool  कर्णफूल  करणबूल\n",
            "tabssum  तबस्सुम  तैस्सम\n",
            "babe  बेब  बेब\n",
            "parivartanshil  परिवर्तनशील  परिवर्तनशील\n",
            "neelaambar  नीलांबर  नीलांफर\n",
            "koshikiya  कोशिकीय  कोशिकिया\n",
            "avarohi  अवरोही  अवरोही\n",
            "nabaleeg  नाबालिग  नबालिल\n",
            "python  पायथन  पिथों\n",
            "kutunmbakam  कुटुंबकम्  कुटुनंजमन\n",
            "nivarniy  निवारणीय  निवारियो\n",
            "sindhugosh  सिंधुघोष  सिंहोग्श\n",
            "mongolian  मंगोलियन  मोंगोलियन\n",
            "jansngh  जनसंघ  जनसंग\n",
            "jalanjali  जलांजलि  जलंल्याई\n",
            "jaaniye  जानिए  जानिये\n",
            "priest  प्रीस्ट  प्रिस्ट\n",
            "piere  पियरे  पायरे\n",
            "khajana  ख़ज़ाना  खजाना\n",
            "besure  बेसुरे  बेसूर\n",
            "shrarton  शरारतों  शररत्रों\n",
            "saptarishi  सप्तर्षि  सप्तऋषि\n",
            "slaai  सलाई  स्लाई\n",
            "robert  रोबर्ट  रॉबर्ट\n",
            "laatey  लाते  लाटे\n",
            "asthetics  एस्थेटिक्स  अस्टेटिक्स\n",
            "dukanen  दुकानें  दुकानें\n",
            "haashmi  हाश्मी  हाशमी\n",
            "farishtey  फरिश्ते  फरिश्टी\n",
            "alagaavavaada  अलगाववाद  अलागववादा\n",
            "domen  डोमेन  डोमें\n",
            "sanskaarit  संस्कारित  संस्कृत\n",
            "prachintam  प्राचीनतम  प्राचिनतम\n",
            "bradli  ब्रैडली  ब्राली\n",
            "odisa  उड़िसा  ऑडिसा\n",
            "rudhir  रूधिर  रुढ़िर\n",
            "akhbaro  अखबारो  अख़बारों\n",
            "pankchar  पंक्चर  पंखचर\n",
            "evo  एवो  ईवो\n",
            "manjile  मंजिले  मांजीले\n",
            "menu  मैन्यू  मेनू\n",
            "likhane  लिखने  लिखाने\n",
            "malikana  मालिकाना  मालिकाना\n",
            "chahchahahat  चहचहाहट  चहचहचाथ्\n",
            "raakh  राख  राख\n",
            "grain  ग्रैन  ग्रेन\n",
            "hawaas  हवास  हवास\n",
            "dal  दल  डाल\n",
            "gangabaai  गंगाबाई  गंगाबाई\n",
            "hatya  हत्था  हत्या\n",
            "bheetar  भीतर  भीतर\n",
            "dilvaaye  दिलवाए  दिलवाएं\n",
            "rengagti  रेंगती  रेंगटिंगे\n",
            "mahashay  महाशय  महाशाय\n",
            "khabbu  खब्बू  खब्बू\n",
            "paran  परण  परण\n",
            "morkkan  मोरक्कन  मोर्क्णा\n",
            "misalein  मिसालें  मिसाइलें\n",
            "etresiya  एट्रेसिया  एटरीयियाई\n",
            "mahashaye  महाशय  महाशाएं\n",
            "project  प्रोजैक्ट  प्रोजेक्ट\n",
            "viprit  विपरीत  विपरित\n",
            "satnaam  सतनाम  सत्नाम\n",
            "suraji  सुराजी  सुरजी\n",
            "associationon  एसोसिएशनों  एसोसिएशनों\n",
            "dimagon  दिमागों  दीमागों\n",
            "sm  एसएम  एसएम\n",
            "congestion  कंजेशन  कांसेशन\n",
            "bariya  बारिया  बारिया\n",
            "vastvikata  वास्तविकता  वास्तविकता\n",
            "answer  आंसर  अंस्वर\n",
            "rajbagh  राजबाग  राजगढ़\n",
            "guptkal  गुप्तकाल  गुप्तकाल\n",
            "vicharane  विचारने  विचारने\n",
            "parasnath  पारसनाथ  परासनोथ\n",
            "chehra  चहरा  चेहरा\n",
            "chiththa  चित्त  चित्ठा\n",
            "najrie  नजरिए  नजरी\n",
            "mansikata  मानसिकता  मानीककता\n",
            "sankalpana  संकल्पना  संकल्पना\n",
            "balaon  बालाओं  बालाओं\n",
            "shuraat  शुरुवात  शुरात\n",
            "shaskiy  शासकीय  शास्किया\n",
            "alex  एलेक्स  अलेक्स\n",
            "algu  अलगू  अल्गू\n",
            "ambar  अंबर  अम्बर\n",
            "boil  बॉयल  बॉयल\n",
            "mahabharat  महाभारत  महाभार्त\n",
            "pulser  पल्सर  पुलर्स\n",
            "haklate  हकलाते  हज्लते\n",
            "balat  बलात  बालत\n",
            "udairaj  उदयराज  उदयराज\n",
            "maurya  मौर्य  मौर्य\n",
            "pucl  पीयूसीएल  पुस्ल\n",
            "maidrik  मौद्रिक  मैद्रिक\n",
            "dhoondhate  ढूंढते  ढूंठाे\n",
            "jiyanpur  जीयनपुर  जीयापुर\n",
            "switched  स्विच्ड  स्वीटेड\n",
            "taraashne  तराशने  तराशने\n",
            "aramco  अरामको  आरामम\n",
            "archer  आर्चर  आर्कर\n",
            "skoop  स्कूप  स्कूप\n",
            "parvaan  परवान  परवान\n",
            "aadhunikikaran  आधुनिकीकरण  आधुनिकरणार\n",
            "kitni  कितनी  कितनी\n",
            "ugaati  उगती  उगती\n",
            "gunbot  गनबोट  गुंबोट\n",
            "chahega  चाहेगा  चाहेगा\n",
            "gandak  गंडक  गंदक\n",
            "krishnam  कृष्णम  कृष्णम\n",
            "aadambaron  आडंबरों  आदंबारों\n",
            "naxalvaadiyon  नक्सलवादियों  नकलवालियों\n",
            "janmashati  जन्मशती  जन्माष्टी\n",
            "achyutanand  अच्युतानंद  अछूत्मंगन\n",
            "utc  यूटीसी  अट\n",
            "sauharda  सौहार्द  सौहार्दा\n",
            "ropad  रोपड़  रोपाड़\n",
            "viewer  व्यूअर  व्यूवर\n",
            "peeskar  पीसकर  पीसकर\n",
            "tareef  तारीफ़  तारीफ\n",
            "bataayegi  बताएगी  बताएगी\n",
            "itemon  आइटमों  इतेनों\n",
            "bodhgya  बोधगया  बोध्या\n",
            "chhupi  छुपी  छुपी\n",
            "week  वीक  वीक\n",
            "jackie  जैकी  जाकी\n",
            "goroun  गोरों  गोरून\n",
            "sarvashaktimaan  सर्वशक्तिमान  सर्वशक्षिततन\n",
            "mahavidyaon  महाविद्याओं  महाविद्यों\n",
            "jame  जामे  जामे\n",
            "etalvi  इतालवी  एतली\n",
            "daivgya  दैवज्ञ  दैवजीया\n",
            "yadi  यदि  यादी\n",
            "laoge  लाओगे  लागे\n",
            "hawao  हवाओ  हवाओं\n",
            "talpade  तलपडे  तलड़पे\n",
            "makode  मकोड़े  मकोड़े\n",
            "dabok  डबोक  दाबोक\n",
            "shararaton  शरारतों  शरारतों\n",
            "phenkane  फेंकने  फेंकने\n",
            "havan  हैवान  हवन\n",
            "cucunur  कुकुनूर  क्सूनुर\n",
            "akash  अकाश  आकाश\n",
            "shira  शीरा  शिरा\n",
            "tennessee  टेनेसी  टेनेसी\n",
            "neeraj  नीरज  नीरज\n",
            "late  लाते  लेट\n",
            "chumban  चुंबन  चुंबन\n",
            "mhadeshiy  महादेशीय  महादेशीय\n",
            "jhonkne  झोंकने  झोंकने\n",
            "ramdan  रमदान  रामदान\n",
            "manjilon  मंजिलों  मंजीलों\n",
            "daal  दाल  दाल\n",
            "parawarti  परवर्ती  परावर्ती\n",
            "dhilapan  ढीलापन  ढिलापन\n",
            "rin  रिन  रान\n",
            "guess  गेस  ग्यूस\n",
            "parlok  परलोक  पार्लकों\n",
            "mx  एमएक्स  एमएस\n",
            "khushiyon  खुशियों  खुशियों\n",
            "samposhit  संपोषित  संमोषित\n",
            "takarata  टकराता  तकारता\n",
            "sg  एसजी  एसजी\n",
            "jarj  जार्ज  जर्ज\n",
            "trim  ट्रिम  त्रिम\n",
            "jhapkaanaa  झपकाना  झापना\n",
            "laaganey  लागने  लागाने\n",
            "maish  मैश  मैश\n",
            "ruchi  रूची  रुचि\n",
            "ubharon  उभारों  उभरों\n",
            "khatak  खटक  खतक\n",
            "nirupamaa  निरुपमा  निरुपमा\n",
            "linga  लिगा  लिंगा\n",
            "pratikramana  प्रतिक्रमण  प्रतिक्रमण\n",
            "mdma  एमडीएमए  एमडाएमए\n",
            "ubhar  उभार  उभर\n",
            "cruck  क्रुक  क्रूक\n",
            "chhodane  छोड़ने  छोड़ने\n",
            "upakaranon  उपकरणों  उपकरणों\n",
            "vaulter  वॉल्टर  वाल्टर\n",
            "dogune  दोगुने  डोग्ने\n",
            "aalishaan  आलिशान  आलीशान\n",
            "jaagane  जागने  जागने\n",
            "rag  राग  रैग\n",
            "lallu  लल्लू  लॉलू\n",
            "karykarta  कार्यकर्ता  कार्यक्रत\n",
            "mooli  मूली  मूली\n",
            "puraanaa  पूराना  पुराना\n",
            "sakka  हक्का  सक्का\n",
            "nmdc  एनएमडीसी  एनएमडीएम\n",
            "tatya  तात्या  तत्य\n",
            "shariri  शरीरी  शरीरी\n",
            "helloveen  हैलोवीन  हेलोइन\n",
            "bhitari  भीतरी  भीतृी\n",
            "ramsay  रामसे  रामसाय\n",
            "notanki  नौटंकी  नोटक्नी\n",
            "ghumte  घूमते  घूमते\n",
            "redel  रिडेल  रेडेल\n",
            "tare  तारे  टारे\n",
            "dhundhlapan  धुंधलापन  धुंधलगा\n",
            "kushan  कुशन  कुशान\n",
            "jodiyaan  जोड़ियां  जोड़ियां\n",
            "sochti  सोचती  सोच्ची\n",
            "nautanki  नौटंकी  नौटकानी\n",
            "manth  मंथ  मांथ\n",
            "reed  रीड  रीद\n",
            "sukma  सुकमा  सुक्म\n",
            "song  सॉन्ग  सोंग\n",
            "rupaye  रूपए  रुपये\n",
            "ed  एड  ईड\n",
            "feltan  फेल्टन  फेलतान\n",
            "sarvshaktiman  सर्वशक्तिमान  सर्वक्षात्खनि\n",
            "shahrikaran  शहरीकरण  शाहीकरण\n",
            "parwarti  परवर्ती  परवरती\n",
            "saha  साहा  सहा\n",
            "cauntron  काउंटरों  कॉन्टरों\n",
            "nikhar  निखार  निखर\n",
            "ugana  उगाना  उगना\n",
            "bhukhe  भूखे  भूखे\n",
            "prativadiyon  प्रतिवादियों  प्रतिवादियों\n",
            "ajadnagar  आजादनगर  अज़ानगर\n",
            "strotra  स्त्रोत्र  स्ट्रोटा\n",
            "khala  खाला  खाला\n",
            "darjan  दर्जन  दरजान\n",
            "takarane  टकराने  तकराने\n",
            "george  जार्ज  ज्यूज\n",
            "lachila  लचीली  लचिला\n",
            "jatilataon  जटिलताओं  जतिलताओं\n",
            "amber  अंबर  एम्बर\n",
            "kundli  कुंडली  कुंदली\n",
            "nikhaaraa  निखारा  निखरा\n",
            "aadambaro  आडंबरों  आदंबार\n",
            "mukhyamantriyon  मुख्यमंत्रियों  मुख्यम्यांिियों\n",
            "hiteck  हाईटेक  हिटेक\n",
            "ojon  ओजोन  ओजो\n",
            "banvae  बनवाए  बनवाए\n",
            "picka  पिका  पिक्का\n",
            "acf  एसीएफ  एसीफी\n",
            "aagarkar  आगरकर  आगरकार\n",
            "rajdroh  राजद्रोह  राजदृो\n",
            "sankhy  संख्य  संख्या\n",
            "pehnana  पहनाना  पहनना\n",
            "loki  लौकी  लोकी\n",
            "hyuar  ह्यूअर  ह्यूर\n",
            "adabi  अदबी  अदाबाई\n",
            "daudane  दौड़ाने  दौड़ने\n",
            "connect  कनेक्ट  कांकेस्ट\n",
            "swati  स्वाती  स्वती\n",
            "loewe  लोई  लोवे\n",
            "akhbar  अखबार  अख़बार\n",
            "badhaaiyaan  बधाईयां  बढ़ाऐं\n",
            "bahadurpur  बहादुरपुर  बहादूर्र\n",
            "tithi  तिथी  तिथि\n",
            "boden  बोडेन  बोड़न\n",
            "arase  अरसे  आरसे\n",
            "chunnani  चुननी  चुनना\n",
            "besraa  बेसरा  बेसरा\n",
            "visangar  विसनगर  विसंगर\n",
            "deghaat  देघाट  देघात\n",
            "rathava  राठवा  राठवा\n",
            "takraata  टकराता  टकराता\n",
            "vinegar  विनेगर  विनेगर\n",
            "jananeta  जननेता  जननेता\n",
            "bradley  ब्रेडले  ब्रैले\n",
            "darias  डेरियस  दरियास\n",
            "cleg  क्लेग  क्लेज\n",
            "dhith  ढीठ  धीठ\n",
            "wahniy  वहनीय  वहनीय\n",
            "kuta  कूटा  कुटा\n",
            "supernova  सुपरनोवा  सुपरनवाद\n",
            "dhani  धनी  धानी\n",
            "jagkar  जागकर  जागकर\n",
            "tirskrit  तिरस्कृत  तीरस्कृत\n",
            "wrapper  रैपर  वाप्रप\n",
            "bastiya  बस्तियां  बस्तिया\n",
            "jansuchna  जनसूचना  जनसूचना\n",
            "anukul  अनुकूल  अनुकुल\n",
            "asther  एस्थर  अस्थर\n",
            "damyanti  दमयंती  दमम्यतं\n",
            "dimaag  दिमाग  दीमाग\n",
            "chase  चेज  चासे\n",
            "hatyaein  हत्याएं  हत्याएं\n",
            "sadashivrao  सदाशिवराव  सदाशिवरण\n",
            "downloding  डाउनलोडिंग  डोनलिंग्PADPADPADPADPADPAD\n",
            "ulte  उल्टे  उल्टे\n",
            "taiyab  तैयब  तैयाब\n",
            "utaraai  उतराई  उताराई\n",
            "jangan  जनगण  जांगन\n",
            "hawas  हवस  हवास\n",
            "raisi  रईसी  राइसी\n",
            "mithilesh  मिथिलेश  मिथिलेश\n",
            "baniae  बनिए  बानिया\n",
            "sahasansthapak  सहसंस्थापक  सहससनक्षक\n",
            "rukawat  रूकावट  रुकावट\n",
            "dribble  ड्रिबल  ड्रिब्ले\n",
            "antena  एंटेना  अंतेना\n",
            "kapolon  कपोलों  कापोलों\n",
            "warrior  वॉरीअर  वेयरो\n",
            "vaanchoo  वांचू  वांचों\n",
            "sarkar  सरकार  सरकार\n",
            "risala  रिसाला  रिसाला\n",
            "restart  रीस्टार्ट  रेस्टर्ट\n",
            "svabhav  स्वभाव  स्वाभाव\n",
            "rahnumai  रहनुमाई  रानुमाई\n",
            "katyayni  कात्यायनी  काट्यानी\n",
            "nets  नेट्स  नेट्स\n",
            "failaakar  फैलाकर  फैलाकार\n",
            "harsh  हर्ष  हर्श\n",
            "insight  इनसाइट  इनसिंत\n",
            "karim  करीम  करिम\n",
            "hasin  हसीन  हासिन\n",
            "stanley  स्टेनले  स्टेनली\n",
            "chalaki  चलाकी  चालीकी\n",
            "netvest  नेटवेस्ट  नेटवेस\n",
            "magazine  मैगजिन  मैगजीन\n",
            "kaagji  कागज़ी  कागजी\n",
            "suljhayaa  सुलझाया  सुलझाया\n",
            "garjan  गर्जन  गर्जन\n",
            "unrasa  अनरसा  उनरासा\n",
            "indu  इंदू  इंदू\n",
            "paapaa  पापा  पापा\n",
            "devidas  देवीदास  देवीदास\n",
            "maiyya  मैया  मैय्य\n",
            "sooje  सूजे  सूजे\n",
            "moresis  माॅरिशस  मोरेसिस\n",
            "pahunchana  पहुंचना  पहुंचाना\n",
            "prastavit  प्रस्तावित  प्रस्ताति\n",
            "neet  नीत  नीत\n",
            "galatfehmi  गलतफहमी  गलतशीमा\n",
            "janneta  जननेता  जनेनता\n",
            "dafnaayi  दफनाई  दफनाए\n",
            "kukunoor  कुकुनूर  कुकूरू\n",
            "bharataa  भरता  भरता\n",
            "birjiya  बिरजिया  बिरजिया\n",
            "rejing  रेजिंग  रेजिंग\n",
            "commity  कमिटी  कमिटी\n",
            "vrutiyon  वृत्तियों  वृतियों\n",
            "jedda  जेद्दा  जेद्दा\n",
            "fija  फिजा  फीजा\n",
            "tejsvi  तेजस्वी  तेजस्वी\n",
            "chaubepur  चौबेपुर  चौबापुर\n",
            "tukka  तुक्का  टुकाका\n",
            "bachate  बचते  बचाते\n",
            "tune  ट्यून  टूने\n",
            "football  फुटबल  फुटबुल\n",
            "nizampur  निजामपुर  निजामपुर\n",
            "kandmool  कंदमूल  कैंडमूल\n",
            "hyman  हायमन  हिमान\n",
            "flat  फॉल्ट  फ्लैट\n",
            "aadat  आदत  आदत\n",
            "sakling  सकलिंग  साल्लिंग\n",
            "dakt  डक्ट  दक्त\n",
            "bareilly  बरेली  बेरेली\n",
            "dashansh  दशांश  दशांश\n",
            "peene  पीने  पीने\n",
            "sanghathan  संघठन  संघठन\n",
            "vritiyon  वृत्तियों  रृतियों\n",
            "process  प्रॉसेस  प्रोसेस\n",
            "meet  मीट  मीट\n",
            "shinta  क्षीणता  शींता\n",
            "namdan  नामदान  नमदन\n",
            "being  बीइंग  बेंग\n",
            "israr  इसरार  इसरार\n",
            "ghud  घुड़  घुड़\n",
            "ghisen  घिसें  घिसेन\n",
            "asankhy  असंख्य  असंख्य\n",
            "os  ओस  ओएस\n",
            "deo  डीईओ  डियो\n",
            "pahnaanaa  पहनाना  पहनाना\n",
            "stritva  स्त्रीत्व  स्त्रीवा\n",
            "doosri  दूसरी  दूसरी\n",
            "kotali  कोटली  कोतली\n",
            "ramnam  रामनाम  रामनाम\n",
            "arnold  अर्नोल्ड  एरनोल्ड\n",
            "conjesion  कंजेशन  कॉन्सियों\n",
            "nivaaran  निवारण  निवारन\n",
            "avaastvik  अवास्तविक  अवश्वतिक\n",
            "classical  क्लासिकल  क्लासिकल\n",
            "badhin  बाघिन  बढ़ियां\n",
            "bhulaakar  भुलाकर  भुलारक\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2SwFlsTuV5l"
      },
      "source": [
        "#Sweep configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQUut1l_nJ4-"
      },
      "source": [
        "sweep_config = {\n",
        "\n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'word_acc',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'embedding_size': {\n",
        "            'values': [16,32,64,128]\n",
        "        },\n",
        "        'num_encoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'num_decoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'hidden_layer_size': {\n",
        "            'values': [32,64,128,256]\n",
        "        },\n",
        "        'cell_type' : {\n",
        "            'values': ['LSTM','GRU','SimpleRNN']  \n",
        "        },\n",
        "        \n",
        "        'drop_out_ratio': {\n",
        "            'values': [0,0.2,0.4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'optimizer':{\n",
        "            'values': ['adam','rmsprop']\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmQgkeQcpKk-"
      },
      "source": [
        "def train():\n",
        "\n",
        "    defaults = dict(\n",
        "    embedding_size = 64,\n",
        "    num_encoder_layers = 1,\n",
        "    num_decoder_layers = 1,\n",
        "    hidden_layer_size = 128,\n",
        "    cell_type = \"LSTM\",\n",
        "    drop_out_ratio = 0,\n",
        "    batch_size = 64,\n",
        "    epochs = 10,\n",
        "    optimizer = 'rmsprop',\n",
        "    )\n",
        "\n",
        "    wandb.init(project=\"CS6910-Assignment3\",config = defaults)\n",
        "    config = wandb.config\n",
        "\n",
        "    hyperparameters = {\n",
        "    \"embedding_size\" : config.embedding_size,\n",
        "    \"num_encoder_layers\" : config.num_encoder_layers,\n",
        "    \"num_decoder_layers\" : config.num_decoder_layers,\n",
        "    \"hidden_layer_size\" : config.hidden_layer_size,\n",
        "    \"cell_type\" : config.cell_type,\n",
        "    \"drop_out_ratio\": config.drop_out_ratio,\n",
        "    \"in_char_size\": len(english_alpha2index),\n",
        "    \"out_char_size\": len(hindi_alpha2index),\n",
        "    \"input_len\": 25,\n",
        "    }\n",
        "\n",
        "    net = RNN(**hyperparameters)\n",
        "    net.compile(optimizer = config.optimizer)\n",
        "    net.fit(integer_encoded_x_train, decoder_input_data,decoder_output_data,batch_size = config.batch_size,epochs = config.epochs)\n",
        "    net.build_inference_model()\n",
        "    combined_validation_data = list(zip(x_raw_val,y_raw_val))\n",
        "    np.random.shuffle(combined_validation_data)\n",
        "    (x_val,y_val) = zip(*combined_validation_data)\n",
        "    x_val_200,y_val_200 = np.array(x_val[:200]),np.array(y_val[:200])\n",
        "    word_accuracy = net.evalute(x_val_200,y_val_200)\n",
        "    wandb.log(({\"word_acc\": word_accuracy}))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCLTVO4Lqf2G",
        "outputId": "118d524e-8bdc-4e6c-947c-79671c0c1dea"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"theindianwriter\", project=\"CS6910-Assignment3\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: fre81alj\n",
            "Sweep URL: https://wandb.ai/theindianwriter/CS6910-Assignment3/sweeps/fre81alj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "507439c4aab6494daa39f06da953f958",
            "05064d08375b4bbeb2f9bc8260230d3c",
            "0c34756a86a34bb08ea1c7704cf1e20d",
            "2c0bad4dd0e44616aafb9bf239c0addc",
            "c6c0ca37474d48749d1f4893ae8d4fac",
            "34d505297a964e4e894956b868209a42",
            "b0b1ea3a2ef54bf18a9805f202c01666",
            "55fdc32faece4dfcbdced312cdb81080"
          ]
        },
        "id": "9p8WeRgcqkXE",
        "outputId": "3bc2299a-dedc-4cfe-d442-238acbec9519"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d7hah238 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: SimpleRNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_out_ratio: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">jolly-sweep-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/theindianwriter/CS6910-Assignment3\" target=\"_blank\">https://wandb.ai/theindianwriter/CS6910-Assignment3</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/theindianwriter/CS6910-Assignment3/sweeps/fre81alj\" target=\"_blank\">https://wandb.ai/theindianwriter/CS6910-Assignment3/sweeps/fre81alj</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/theindianwriter/CS6910-Assignment3/runs/d7hah238\" target=\"_blank\">https://wandb.ai/theindianwriter/CS6910-Assignment3/runs/d7hah238</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210522_132340-d7hah238</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "622/622 [==============================] - 33s 50ms/step - loss: 1.1872 - accuracy: 0.7342 - val_loss: 0.8955 - val_accuracy: 0.7658\n",
            "Epoch 2/25\n",
            "622/622 [==============================] - 31s 50ms/step - loss: 0.7961 - accuracy: 0.7850 - val_loss: 0.9001 - val_accuracy: 0.7670\n",
            "Epoch 3/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.7715 - accuracy: 0.7905 - val_loss: 1.2968 - val_accuracy: 0.7639\n",
            "Epoch 4/25\n",
            "622/622 [==============================] - 30s 48ms/step - loss: 0.7755 - accuracy: 0.7904 - val_loss: 0.9238 - val_accuracy: 0.7684\n",
            "Epoch 5/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.7356 - accuracy: 0.7962 - val_loss: 0.9239 - val_accuracy: 0.7696\n",
            "Epoch 6/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.7277 - accuracy: 0.7976 - val_loss: 0.9220 - val_accuracy: 0.7698\n",
            "Epoch 7/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.7202 - accuracy: 0.8002 - val_loss: 0.9419 - val_accuracy: 0.7711\n",
            "Epoch 8/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.7080 - accuracy: 0.8018 - val_loss: 1.0256 - val_accuracy: 0.7669\n",
            "Epoch 9/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.7234 - accuracy: 0.8011 - val_loss: 0.9757 - val_accuracy: 0.7731\n",
            "Epoch 10/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.6971 - accuracy: 0.8050 - val_loss: 0.9874 - val_accuracy: 0.7737\n",
            "Epoch 11/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.6873 - accuracy: 0.8074 - val_loss: 0.9777 - val_accuracy: 0.7711\n",
            "Epoch 12/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.6864 - accuracy: 0.8073 - val_loss: 1.0006 - val_accuracy: 0.7699\n",
            "Epoch 13/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.6757 - accuracy: 0.8103 - val_loss: 1.0091 - val_accuracy: 0.7713\n",
            "Epoch 14/25\n",
            "622/622 [==============================] - 30s 48ms/step - loss: 0.6704 - accuracy: 0.8114 - val_loss: 1.0131 - val_accuracy: 0.7709\n",
            "Epoch 15/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.6731 - accuracy: 0.8117 - val_loss: 1.0042 - val_accuracy: 0.7673\n",
            "Epoch 16/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.6878 - accuracy: 0.8079 - val_loss: 1.0181 - val_accuracy: 0.7698\n",
            "Epoch 17/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.6666 - accuracy: 0.8127 - val_loss: 1.0278 - val_accuracy: 0.7708\n",
            "Epoch 18/25\n",
            "622/622 [==============================] - 31s 50ms/step - loss: 0.6593 - accuracy: 0.8145 - val_loss: 1.0338 - val_accuracy: 0.7705\n",
            "Epoch 19/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.6544 - accuracy: 0.8156 - val_loss: 1.0483 - val_accuracy: 0.7704\n",
            "Epoch 20/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.6518 - accuracy: 0.7942 - val_loss: 1.0392 - val_accuracy: 0.7292\n",
            "Epoch 21/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.6470 - accuracy: 0.7778 - val_loss: 1.0764 - val_accuracy: 0.7305\n",
            "Epoch 22/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.6473 - accuracy: 0.7803 - val_loss: 1.0551 - val_accuracy: 0.7279\n",
            "Epoch 23/25\n",
            "622/622 [==============================] - 30s 49ms/step - loss: 0.6405 - accuracy: 0.7793 - val_loss: 1.0571 - val_accuracy: 0.7279\n",
            "Epoch 24/25\n",
            "622/622 [==============================] - 31s 49ms/step - loss: 0.6376 - accuracy: 0.7799 - val_loss: 1.0822 - val_accuracy: 0.7297\n",
            "Epoch 25/25\n",
            "622/622 [==============================] - 31s 50ms/step - loss: 0.6366 - accuracy: 0.7798 - val_loss: 1.0908 - val_accuracy: 0.7227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1363<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "507439c4aab6494daa39f06da953f958",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 1.23MB of 1.23MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210522_132340-d7hah238/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210522_132340-d7hah238/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>24</td></tr><tr><td>loss</td><td>0.63758</td></tr><tr><td>accuracy</td><td>0.77967</td></tr><tr><td>val_loss</td><td>1.09075</td></tr><tr><td>val_accuracy</td><td>0.72267</td></tr><tr><td>_runtime</td><td>768</td></tr><tr><td>_timestamp</td><td>1621690588</td></tr><tr><td>_step</td><td>24</td></tr><tr><td>best_val_loss</td><td>0.89554</td></tr><tr><td>best_epoch</td><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>loss</td><td>█▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▄▅▅▅▆▆▆▆▇▇▇▇█▇▇███▄▃▃▃▃▃</td></tr><tr><td>val_loss</td><td>▁▁█▁▁▁▂▃▂▃▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄</td></tr><tr><td>val_accuracy</td><td>▇▇▇▇▇▇█▇███▇██▇▇███▂▂▂▂▂▁</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">jolly-sweep-7</strong>: <a href=\"https://wandb.ai/theindianwriter/CS6910-Assignment3/runs/d7hah238\" target=\"_blank\">https://wandb.ai/theindianwriter/CS6910-Assignment3/runs/d7hah238</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dg7f27lc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_out_ratio: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">golden-sweep-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/theindianwriter/CS6910-Assignment3\" target=\"_blank\">https://wandb.ai/theindianwriter/CS6910-Assignment3</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/theindianwriter/CS6910-Assignment3/sweeps/fre81alj\" target=\"_blank\">https://wandb.ai/theindianwriter/CS6910-Assignment3/sweeps/fre81alj</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/theindianwriter/CS6910-Assignment3/runs/dg7f27lc\" target=\"_blank\">https://wandb.ai/theindianwriter/CS6910-Assignment3/runs/dg7f27lc</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210522_133633-dg7f27lc</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            " 694/1244 [===============>..............] - ETA: 8s - loss: 1.2912 - accuracy: 0.7358"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y78nDt2uk8r0"
      },
      "source": [
        "#A single runnable Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmTV6xIxghXx"
      },
      "source": [
        "batch_size = 64  \n",
        "epochs = 25\n",
        "latent_dim = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm_j9pmRb8oE"
      },
      "source": [
        "embedding_inputs = keras.Input(shape=(25))\n",
        "embedding_layer = Embedding(len(english_alpha2index), 125 , input_length=25)\n",
        "encoder_inputs = embedding_layer(embedding_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, len(hindi_alpha2index)))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(hindi_alpha2index), activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([embedding_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RD1E7QDgwoq",
        "outputId": "8e3db823-f277-4f6c-a4a6-474e70d9fecb"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [integer_encoded_x_train, decoder_input_data],\n",
        "    decoder_output_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "622/622 [==============================] - 191s 302ms/step - loss: 1.1451 - accuracy: 0.7386 - val_loss: 0.9244 - val_accuracy: 0.7665\n",
            "Epoch 2/25\n",
            "622/622 [==============================] - 201s 323ms/step - loss: 0.7544 - accuracy: 0.8006 - val_loss: 0.8717 - val_accuracy: 0.7954\n",
            "Epoch 3/25\n",
            "622/622 [==============================] - 185s 297ms/step - loss: 0.6064 - accuracy: 0.8345 - val_loss: 0.7435 - val_accuracy: 0.8184\n",
            "Epoch 4/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.4774 - accuracy: 0.8660 - val_loss: 0.6884 - val_accuracy: 0.8375\n",
            "Epoch 5/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.3836 - accuracy: 0.8901 - val_loss: 0.6404 - val_accuracy: 0.8472\n",
            "Epoch 6/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.3134 - accuracy: 0.9090 - val_loss: 0.5655 - val_accuracy: 0.8677\n",
            "Epoch 7/25\n",
            "622/622 [==============================] - 184s 296ms/step - loss: 0.2615 - accuracy: 0.9227 - val_loss: 0.5317 - val_accuracy: 0.8723\n",
            "Epoch 8/25\n",
            "622/622 [==============================] - 185s 297ms/step - loss: 0.2206 - accuracy: 0.9350 - val_loss: 0.5339 - val_accuracy: 0.8776\n",
            "Epoch 9/25\n",
            "622/622 [==============================] - 183s 294ms/step - loss: 0.1930 - accuracy: 0.9429 - val_loss: 0.5209 - val_accuracy: 0.8845\n",
            "Epoch 10/25\n",
            "622/622 [==============================] - 183s 295ms/step - loss: 0.1706 - accuracy: 0.9491 - val_loss: 0.4781 - val_accuracy: 0.8855\n",
            "Epoch 11/25\n",
            "622/622 [==============================] - 183s 295ms/step - loss: 0.1521 - accuracy: 0.9542 - val_loss: 0.4634 - val_accuracy: 0.8938\n",
            "Epoch 12/25\n",
            "622/622 [==============================] - 183s 295ms/step - loss: 0.1384 - accuracy: 0.9583 - val_loss: 0.4890 - val_accuracy: 0.8879\n",
            "Epoch 13/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.1249 - accuracy: 0.9627 - val_loss: 0.4433 - val_accuracy: 0.8958\n",
            "Epoch 14/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.1146 - accuracy: 0.9659 - val_loss: 0.4897 - val_accuracy: 0.8914\n",
            "Epoch 15/25\n",
            "622/622 [==============================] - 184s 296ms/step - loss: 0.1063 - accuracy: 0.9685 - val_loss: 0.4902 - val_accuracy: 0.8905\n",
            "Epoch 16/25\n",
            "622/622 [==============================] - 185s 297ms/step - loss: 0.1007 - accuracy: 0.9712 - val_loss: 0.4842 - val_accuracy: 0.8942\n",
            "Epoch 17/25\n",
            "622/622 [==============================] - 184s 296ms/step - loss: 0.0983 - accuracy: 0.9737 - val_loss: 0.4617 - val_accuracy: 0.8967\n",
            "Epoch 18/25\n",
            "622/622 [==============================] - 183s 295ms/step - loss: 0.0950 - accuracy: 0.9756 - val_loss: 0.4782 - val_accuracy: 0.8956\n",
            "Epoch 19/25\n",
            "622/622 [==============================] - 183s 295ms/step - loss: 0.0876 - accuracy: 0.9776 - val_loss: 0.5064 - val_accuracy: 0.8938\n",
            "Epoch 20/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.0774 - accuracy: 0.9795 - val_loss: 0.5035 - val_accuracy: 0.8949\n",
            "Epoch 21/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.0707 - accuracy: 0.9809 - val_loss: 0.5102 - val_accuracy: 0.8951\n",
            "Epoch 22/25\n",
            "622/622 [==============================] - 184s 295ms/step - loss: 0.0650 - accuracy: 0.9823 - val_loss: 0.5384 - val_accuracy: 0.8942\n",
            "Epoch 23/25\n",
            "622/622 [==============================] - 183s 295ms/step - loss: 0.0607 - accuracy: 0.9835 - val_loss: 0.5279 - val_accuracy: 0.8941\n",
            "Epoch 24/25\n",
            "622/622 [==============================] - 183s 294ms/step - loss: 0.0559 - accuracy: 0.9848 - val_loss: 0.5401 - val_accuracy: 0.8934\n",
            "Epoch 25/25\n",
            "622/622 [==============================] - 183s 295ms/step - loss: 0.0517 - accuracy: 0.9857 - val_loss: 0.5410 - val_accuracy: 0.8927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4f51e80c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P78a0OjtWnp",
        "outputId": "6f98529d-e2af-4f7e-e65d-1f97f1d843d0"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/s2s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_22_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn, lstm_cell_23_layer_call_and_return_conditional_losses, lstm_cell_23_layer_call_fn, lstm_cell_22_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_22_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn, lstm_cell_23_layer_call_and_return_conditional_losses, lstm_cell_23_layer_call_fn, lstm_cell_22_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/s2s/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/s2s/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8PbtJSVFUpl"
      },
      "source": [
        "model = keras.models.load_model(\"/content/drive/MyDrive/s2s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNd1XFY3UGO5",
        "outputId": "5071a31e-da30-4f3f-c0ba-84fe75ad10a6"
      },
      "source": [
        "#model.summary()\n",
        "\n",
        "embedding_inputs = model.input[0]  # input_1\n",
        "embedding_l1 = model.layers[1]\n",
        "encoder_inputs = embedding_l1(embedding_inputs)\n",
        "encoder_lstm = model.layers[3]\n",
        "encoder_outputs, state_h_enc, state_c_enc = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(embedding_inputs, encoder_states)\n",
        "encoder_model.summary()\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_5\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[4]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[5]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "decoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, 25)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 25, 125)           3375      \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               [(None, 256), (None, 256) 391168    \n",
            "=================================================================\n",
            "Total params: 394,543\n",
            "Trainable params: 394,543\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           [(None, None, 131)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  [(None, None, 256),  397312      input_12[0][0]                   \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, None, 131)    33667       lstm_11[2][0]                    \n",
            "==================================================================================================\n",
            "Total params: 430,979\n",
            "Trainable params: 430,979\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCIjA1GQmggp"
      },
      "source": [
        "def decode_sequence(word):\n",
        "    input_seq = get_integer_encode(word,english_alpha2index)\n",
        "    input_seq = input_seq.reshape(1,25)\n",
        "    \n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1, len(hindi_alpha2index)))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0,0, hindi_alpha2index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_word = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        sampled_char = hindi_index2alpha[sampled_token_index]\n",
        "        \n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_word) > 25:\n",
        "            stop_condition = True\n",
        "            break\n",
        "\n",
        "        decoded_word += sampled_char\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1,len(hindi_alpha2index)))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-LRyPGcu91g"
      },
      "source": [
        "def test():\n",
        "    correct = 0\n",
        "    for english_word,hindi_word in zip(x_raw_test,y_raw_test):\n",
        "    \n",
        "        predicted_hindi_word = decode_sequence(english_word)\n",
        "        print(english_word+\"  \"+hindi_word+\"  \"+predicted_hindi_word)\n",
        "        if predicted_hindi_word == hindi_word:\n",
        "            correct += 1\n",
        "\n",
        "    acc = (correct/x_raw_test.shape[0])*100\n",
        "    return acc\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}