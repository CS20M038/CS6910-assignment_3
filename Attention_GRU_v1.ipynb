{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled42.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CS20M038/CS6910-assignment_3/blob/main/Attention_GRU_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gTF5hZaWvyR"
      },
      "source": [
        "#Start Creating the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9FRhJZG4Tlj"
      },
      "source": [
        "#@title Import Libraries\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import models\n",
        "from numpy import array_equal\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from scipy.ndimage.interpolation import shift\n",
        "import csv\n",
        "import random"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIaBBs1MQ3t_",
        "outputId": "9a058f8e-8977-4a20-9781-32c857faa98c"
      },
      "source": [
        "#@title Check GPU\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd9Sf21CxKd6",
        "outputId": "dddfed59-a19c-4f0e-a7c3-ad7d6b7bdcfb"
      },
      "source": [
        "input_dict=['<start>','<end>']\n",
        "target_dict=['<start>','<end>']\n",
        "eng_alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
        "pad_char = '<end>'\n",
        "eng_alpha2index = {'<start>': 0,'<end>':1}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "    eng_alpha2index[alpha] = index+2\n",
        "    input_dict.append(alpha)\n",
        "\n",
        "print(eng_alpha2index)\n",
        "print(input_dict)\n",
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_alpha2index = {'<start>': 0,pad_char: 1}\n",
        "#hindi_alpha2index = {'<start>': 0}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "    hindi_alpha2index[alpha] = index+2\n",
        "    target_dict.append(alpha)\n",
        "    #print(alpha)\n",
        "\n",
        "print(hindi_alpha2index)\n",
        "print(target_dict)\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<start>': 0, '<end>': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "['<start>', '<end>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "{'<start>': 0, '<end>': 1, 'ऀ': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'ऄ': 6, 'अ': 7, 'आ': 8, 'इ': 9, 'ई': 10, 'उ': 11, 'ऊ': 12, 'ऋ': 13, 'ऌ': 14, 'ऍ': 15, 'ऎ': 16, 'ए': 17, 'ऐ': 18, 'ऑ': 19, 'ऒ': 20, 'ओ': 21, 'औ': 22, 'क': 23, 'ख': 24, 'ग': 25, 'घ': 26, 'ङ': 27, 'च': 28, 'छ': 29, 'ज': 30, 'झ': 31, 'ञ': 32, 'ट': 33, 'ठ': 34, 'ड': 35, 'ढ': 36, 'ण': 37, 'त': 38, 'थ': 39, 'द': 40, 'ध': 41, 'न': 42, 'ऩ': 43, 'प': 44, 'फ': 45, 'ब': 46, 'भ': 47, 'म': 48, 'य': 49, 'र': 50, 'ऱ': 51, 'ल': 52, 'ळ': 53, 'ऴ': 54, 'व': 55, 'श': 56, 'ष': 57, 'स': 58, 'ह': 59, 'ऺ': 60, 'ऻ': 61, '़': 62, 'ऽ': 63, 'ा': 64, 'ि': 65, 'ी': 66, 'ु': 67, 'ू': 68, 'ृ': 69, 'ॄ': 70, 'ॅ': 71, 'ॆ': 72, 'े': 73, 'ै': 74, 'ॉ': 75, 'ॊ': 76, 'ो': 77, 'ौ': 78, '्': 79, 'ॎ': 80, 'ॏ': 81, 'ॐ': 82, '॑': 83, '॒': 84, '॓': 85, '॔': 86, 'ॕ': 87, 'ॖ': 88, 'ॗ': 89, 'क़': 90, 'ख़': 91, 'ग़': 92, 'ज़': 93, 'ड़': 94, 'ढ़': 95, 'फ़': 96, 'य़': 97, 'ॠ': 98, 'ॡ': 99, 'ॢ': 100, 'ॣ': 101, '।': 102, '॥': 103, '०': 104, '१': 105, '२': 106, '३': 107, '४': 108, '५': 109, '६': 110, '७': 111, '८': 112, '९': 113, '॰': 114, 'ॱ': 115, 'ॲ': 116, 'ॳ': 117, 'ॴ': 118, 'ॵ': 119, 'ॶ': 120, 'ॷ': 121, 'ॸ': 122, 'ॹ': 123, 'ॺ': 124, 'ॻ': 125, 'ॼ': 126, 'ॽ': 127, 'ॾ': 128, 'ॿ': 129}\n",
            "['<start>', '<end>', 'ऀ', 'ँ', 'ं', 'ः', 'ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', 'स', 'ह', 'ऺ', 'ऻ', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'ॎ', 'ॏ', 'ॐ', '॑', '॒', '॓', '॔', 'ॕ', 'ॖ', 'ॗ', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', 'ॠ', 'ॡ', 'ॢ', 'ॣ', '।', '॥', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '॰', 'ॱ', 'ॲ', 'ॳ', 'ॴ', 'ॵ', 'ॶ', 'ॷ', 'ॸ', 'ॹ', 'ॺ', 'ॻ', 'ॼ', 'ॽ', 'ॾ', 'ॿ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gSSbLgcxKd7",
        "outputId": "e00e67d4-25f7-44ea-dff6-a5622d7aa24b"
      },
      "source": [
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_index2alpha = {0:'<start>',1: pad_char}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "    hindi_index2alpha[index+2] = alpha\n",
        "   # target_dict.append(alpha)\n",
        "    #print(alpha)\n",
        "\n",
        "print(hindi_index2alpha)\n",
        "\n",
        "eng_alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
        "eng_index2alpha = {0:'<start>',1: pad_char}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "    eng_index2alpha[index+2] = alpha\n",
        "   # target_dict.append(alpha)\n",
        "    #print(alpha)\n",
        "\n",
        "print(eng_index2alpha)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '<start>', 1: '<end>', 2: 'ऀ', 3: 'ँ', 4: 'ं', 5: 'ः', 6: 'ऄ', 7: 'अ', 8: 'आ', 9: 'इ', 10: 'ई', 11: 'उ', 12: 'ऊ', 13: 'ऋ', 14: 'ऌ', 15: 'ऍ', 16: 'ऎ', 17: 'ए', 18: 'ऐ', 19: 'ऑ', 20: 'ऒ', 21: 'ओ', 22: 'औ', 23: 'क', 24: 'ख', 25: 'ग', 26: 'घ', 27: 'ङ', 28: 'च', 29: 'छ', 30: 'ज', 31: 'झ', 32: 'ञ', 33: 'ट', 34: 'ठ', 35: 'ड', 36: 'ढ', 37: 'ण', 38: 'त', 39: 'थ', 40: 'द', 41: 'ध', 42: 'न', 43: 'ऩ', 44: 'प', 45: 'फ', 46: 'ब', 47: 'भ', 48: 'म', 49: 'य', 50: 'र', 51: 'ऱ', 52: 'ल', 53: 'ळ', 54: 'ऴ', 55: 'व', 56: 'श', 57: 'ष', 58: 'स', 59: 'ह', 60: 'ऺ', 61: 'ऻ', 62: '़', 63: 'ऽ', 64: 'ा', 65: 'ि', 66: 'ी', 67: 'ु', 68: 'ू', 69: 'ृ', 70: 'ॄ', 71: 'ॅ', 72: 'ॆ', 73: 'े', 74: 'ै', 75: 'ॉ', 76: 'ॊ', 77: 'ो', 78: 'ौ', 79: '्', 80: 'ॎ', 81: 'ॏ', 82: 'ॐ', 83: '॑', 84: '॒', 85: '॓', 86: '॔', 87: 'ॕ', 88: 'ॖ', 89: 'ॗ', 90: 'क़', 91: 'ख़', 92: 'ग़', 93: 'ज़', 94: 'ड़', 95: 'ढ़', 96: 'फ़', 97: 'य़', 98: 'ॠ', 99: 'ॡ', 100: 'ॢ', 101: 'ॣ', 102: '।', 103: '॥', 104: '०', 105: '१', 106: '२', 107: '३', 108: '४', 109: '५', 110: '६', 111: '७', 112: '८', 113: '९', 114: '॰', 115: 'ॱ', 116: 'ॲ', 117: 'ॳ', 118: 'ॴ', 119: 'ॵ', 120: 'ॶ', 121: 'ॷ', 122: 'ॸ', 123: 'ॹ', 124: 'ॺ', 125: 'ॻ', 126: 'ॼ', 127: 'ॽ', 128: 'ॾ', 129: 'ॿ'}\n",
            "{0: '<start>', 1: '<end>', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ0aXzuU5ifS"
      },
      "source": [
        "def readXmlDataset(filename):\n",
        "        tsv_file = open(filename)\n",
        "        read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
        "        lang1_words = []\n",
        "        lang2_words = []\n",
        "\n",
        "        for row in read_tsv:\n",
        "            lang2_words.append(row[0])\n",
        "            lang1_words.append(row[1])\n",
        "            #print(row[0])\n",
        "\n",
        "        return lang1_words, lang2_words\n",
        "train_input_texts, train_target_texts = readXmlDataset('hi.translit.sampled.train.tsv')\n",
        "test_input_texts, test_target_texts = readXmlDataset('hi.translit.sampled.test.tsv')"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WP3VgZ-oYKFV",
        "outputId": "bdaee4d3-c858-4de5-92fb-acfef876d8ac"
      },
      "source": [
        "train_input_texts[0]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'an'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYtIFIHz634J"
      },
      "source": [
        "def word_rep(word, letter2index, device = 'cpu'):\n",
        "    rep = np.zeros((len(word)+1,  129))\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        rep[letter_index][pos] = 1\n",
        "    pad_pos = letter2index[pad_char]\n",
        "    rep[letter_index+1][pad_pos] = 1\n",
        "    return rep\n",
        "\n",
        "def gt_rep(word, letter2index, device = 'cpu'):\n",
        "    gt_rep = np.zeros([len(word)+2], dtype=np.long)\n",
        "    gt_rep[0]=0\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        gt_rep[letter_index+1] = pos\n",
        "    gt_rep[letter_index+2]= letter2index[pad_char]\n",
        "    return gt_rep\n",
        "    word_rep('abc',eng_alpha2index).shape\n",
        "    x=gt_rep('abc',eng_alpha2index)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBwyPEeZXO9R",
        "outputId": "61ac6d7c-d938-46e0-e339-6aefd7ca9cdf"
      },
      "source": [
        "gt_rep('abc',eng_alpha2index)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 3, 4, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpXZ36j_5K3Y"
      },
      "source": [
        "def createDataset(dataset,letter2index):\n",
        "    X_train=[]\n",
        "    for i,data in enumerate(dataset):\n",
        "        #print(data)\n",
        "        X_train.append(gt_rep(data,letter2index))\n",
        "    X_train = np.array(X_train)\n",
        "    return X_train"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ5892ABXs4Z",
        "outputId": "7c25745e-da1c-4107-ba13-9eac2d1a0556"
      },
      "source": [
        "X_train = createDataset(train_input_texts,eng_alpha2index)\n",
        "y_train = createDataset(train_target_texts,hindi_alpha2index)\n",
        "X_test = createDataset(test_input_texts,eng_alpha2index)\n",
        "y_test = createDataset(test_target_texts,hindi_alpha2index)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW1ewK688Cfg",
        "outputId": "aa99f882-6b68-4e73-fda6-d5d65be65320"
      },
      "source": [
        "print(len(eng_alpha2index))\n",
        "#a=[0]*(len(eng_alpha2index)-1)\n",
        "a=[0]*128\n",
        "eng_pad=[1]\n",
        "eng_pad=eng_pad+a\n",
        "print(eng_pad)\n",
        "print(len(hindi_alpha2index))\n",
        "a=[0]*128\n",
        "hindi_pad=[1]\n",
        "hindi_pad=hindi_pad+a\n",
        "print(hindi_pad)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "129\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nczkBT69KdWd"
      },
      "source": [
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH7o-dfEKxHf",
        "outputId": "9daa0bc7-1977-4d95-bf19-b0499552aa91"
      },
      "source": [
        "one_hot_encode([1,2,3],10)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ABSEIPIcGLnX",
        "outputId": "eb1f2e74-9ab9-4bf4-b3e5-c753a51884e4"
      },
      "source": [
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        "'''d=one_hot_decode(y_train_padded[5])\n",
        "c=hindi_index2alpha[6]+hindi_index2alpha[3]+hindi_index2alpha[22]+hindi_index2alpha[66]+hindi_index2alpha[49]+hindi_index2alpha[64]+hindi_index2alpha[37]+hindi_index2alpha[0]\n",
        "c'''"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'d=one_hot_decode(y_train_padded[5])\\nc=hindi_index2alpha[6]+hindi_index2alpha[3]+hindi_index2alpha[22]+hindi_index2alpha[66]+hindi_index2alpha[49]+hindi_index2alpha[64]+hindi_index2alpha[37]+hindi_index2alpha[0]\\nc'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSce-sL4HWTr",
        "outputId": "d1567dd0-4ee8-4c69-fe5d-d979bf69a865"
      },
      "source": [
        "max_input_sequence= max(len(seq) for seq in X_train)\n",
        "max_output_sequence= max(len(seq) for seq in y_train)\n",
        "\n",
        "print('max_input_sequence: ', max_input_sequence)\n",
        "print('max_output_sequence: ', max_output_sequence)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_input_sequence:  22\n",
            "max_output_sequence:  21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2abm43MfHWWG",
        "outputId": "5952539c-1e4f-4a41-9df2-2e6966dce698"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X_train_padded = pad_sequences(X_train, maxlen= max_input_sequence, padding='post', value=1)\n",
        "print(\"X_train shape: \",X_train.shape)\n",
        "print(\"X_train_padded shape: \",X_train_padded.shape)\n",
        "\n",
        "y_train_padded = pad_sequences(y_train, maxlen= max_output_sequence, padding='post', value=1)\n",
        "print(\"y_train shape: \",y_train.shape)\n",
        "print(\"y_train_padded shape: \",y_train_padded.shape)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (44204,)\n",
            "X_train_padded shape:  (44204, 22)\n",
            "y_train shape:  (44204,)\n",
            "y_train_padded shape:  (44204, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvw10trdHWY8",
        "outputId": "20b2d5b8-df8b-45a6-f89a-5bef9ecc29c5"
      },
      "source": [
        "i=0\n",
        "print(\"____Sample Input (Raw Format)____\")\n",
        "print(\"Original:\\n\", one_hot_decode(X_train[i]))\n",
        "print(\"Padded:\\n\",one_hot_decode(X_train_padded[i]))\n",
        "print(\"____Corresponding Output (Raw Format)____\")\n",
        "print(\"Original:\\n\", one_hot_decode(y_train[i]))\n",
        "print(\"Padded:\\n\",one_hot_decode(y_train_padded[i]))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "____Sample Input (Raw Format)____\n",
            "Original:\n",
            " [0, 0, 0, 0]\n",
            "Padded:\n",
            " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "____Corresponding Output (Raw Format)____\n",
            "Original:\n",
            " [0, 0, 0, 0]\n",
            "Padded:\n",
            " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN97C-THHWbb",
        "outputId": "231a1c46-6896-4f90-e806-caa67af53fb3"
      },
      "source": [
        "X_test_padded = pad_sequences(X_test, maxlen= max_input_sequence, padding='post', \n",
        "                              value=0)\n",
        "print(\"X_test shape: \",X_test.shape)\n",
        "print(\"X_test_padded shape: \",X_test_padded.shape)\n",
        "\n",
        "y_test_padded = pad_sequences(y_test, maxlen= max_output_sequence, padding='post', \n",
        "                              value=0)\n",
        "print(\"y_test shape: \",y_test.shape)\n",
        "print(\"y_test_padded shape: \",y_test_padded.shape)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_test shape:  (4502,)\n",
            "X_test_padded shape:  (4502, 22)\n",
            "y_test shape:  (4502,)\n",
            "y_test_padded shape:  (4502, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFu_u0dihpds",
        "outputId": "14e0d4a2-b86a-41da-87a5-1dc282784812"
      },
      "source": [
        "!pip3 install sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "input_tensor, target_tensor = X_train_padded, y_train_padded\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq7lBv-AomLf"
      },
      "source": [
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqOzSwkVopNU",
        "outputId": "10fbd925-7196-4dc0-cdcc-32dc54bb9017"
      },
      "source": [
        "max_length_targ"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_5iG58-h8fh",
        "outputId": "6d37561d-6f7a-4855-872f-de6dbaa7dc5a"
      },
      "source": [
        "target_tensor_train[0]"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 35, 75, 42, 50,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61UG3bP5htop",
        "outputId": "487bc17d-d2f6-4ab2-cd9c-2e6b8d1920c8"
      },
      "source": [
        "# Show the mapping b/w word index and language tokenizer\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0 and t!=1:\n",
        "      print (\"%d ----> %s\" % (t, lang[t]))\n",
        "      \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(eng_index2alpha, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(hindi_index2alpha, target_tensor_train[0])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "5 ----> d\n",
            "2 ----> a\n",
            "24 ----> w\n",
            "15 ----> n\n",
            "6 ----> e\n",
            "19 ----> r\n",
            "\n",
            "Target Language; index to word mapping\n",
            "35 ----> ड\n",
            "75 ----> ॉ\n",
            "42 ----> न\n",
            "50 ----> र\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkJs7SH4iUee",
        "outputId": "57b0f982-db90-4756-b166-bf861d7d97b1"
      },
      "source": [
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35363 35363 8841 8841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9zTfV1EhVR5"
      },
      "source": [
        "#End creating data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdsVStEahZir"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXpM1I-yOAPi"
      },
      "source": [
        "# Essential model parameters\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(eng_alpha2index) + 1\n",
        "vocab_tar_size = len(hindi_alpha2index) + 1"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbWzOakDOpCn"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsDobIhnODno",
        "outputId": "0b11f1bc-ca56-4d0b-c24d-df413c45f40a"
      },
      "source": [
        "# Size of input and target batches\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([256, 22]), TensorShape([256, 21]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di-Pl2VDOwUB"
      },
      "source": [
        "# Encoder class\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "\n",
        "    # Embed the vocab to a dense embedding \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # GRU Layer\n",
        "    # glorot_uniform: Initializer for the recurrent_kernel weights matrix, \n",
        "    # used for the linear transformation of the recurrent state\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  # Encoder network comprises an Embedding layer followed by a GRU layer\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  # To initialize the hidden state\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw3ZKssrOxT1",
        "outputId": "b29adaba-14e5-44d2-9f0d-083050263659"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (256, 22, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (256, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EgJFFbHO1kQ"
      },
      "source": [
        "# Attention Mechanism\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KphwlHYO-5S",
        "outputId": "d6b398a3-1298-4720-f4ad-b51266e638b7"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (256, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (256, 22, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc9CUy7VO_ua"
      },
      "source": [
        "# Decoder class\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # x shape == (batch_size, 1)\n",
        "    # hidden shape == (batch_size, max_length)\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # context_vector shape == (batch_size, hidden_size)\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9VmxMpHPMig",
        "outputId": "d5b8e0fd-ee56-4bb9-fa3b-b490c6a08577"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (256, 131)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOuNXoeDPU6q"
      },
      "source": [
        "# Initialize optimizer and loss functions\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# Loss function\n",
        "def loss_function(real, pred):\n",
        "\n",
        "  # Take care of the padding. Not all sequences are of equal length.\n",
        "  # If there's a '0' in the sequence, the loss is being nullified\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZwpu5cYPZHK"
      },
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVQe7NwKPaCc"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  # tf.GradientTape() -- record operations for automatic differentiation\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # dec_hidden is used by attention, hence is the same enc_hidden\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # <start> token is the initial decoder input\n",
        "    dec_input = tf.expand_dims([0] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "\n",
        "      # Pass enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # Use teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # As this function is called per batch, compute the batch_loss\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  # Get the model's variables\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  # Compute the gradients\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # Update the variables of the model/network\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM31H6RYPerQ",
        "outputId": "f9ee16e0-3fb1-44d7-f156-cf4b448eabea"
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # Initialize the hidden state\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  # Loop through the dataset\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "\n",
        "    # Call the train method\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "\n",
        "    # Compute the loss (per batch)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # Save (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  # Output the loss observed until that epoch\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  \n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7930\n",
            "Epoch 1 Batch 100 Loss 0.6325\n",
            "Epoch 1 Loss 0.6866\n",
            "Time taken for 1 epoch 107.59599137306213 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5015\n",
            "Epoch 2 Batch 100 Loss 0.2630\n",
            "Epoch 2 Loss 0.3252\n",
            "Time taken for 1 epoch 107.74142003059387 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2333\n",
            "Epoch 3 Batch 100 Loss 0.2126\n",
            "Epoch 3 Loss 0.2027\n",
            "Time taken for 1 epoch 107.83285856246948 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1733\n",
            "Epoch 4 Batch 100 Loss 0.1511\n",
            "Epoch 4 Loss 0.1633\n",
            "Time taken for 1 epoch 107.71837520599365 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1360\n",
            "Epoch 5 Batch 100 Loss 0.1350\n",
            "Epoch 5 Loss 0.1455\n",
            "Time taken for 1 epoch 107.1535210609436 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1307\n",
            "Epoch 6 Batch 100 Loss 0.1263\n",
            "Epoch 6 Loss 0.1246\n",
            "Time taken for 1 epoch 107.61810088157654 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1203\n",
            "Epoch 7 Batch 100 Loss 0.1086\n",
            "Epoch 7 Loss 0.1093\n",
            "Time taken for 1 epoch 107.311283826828 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0845\n",
            "Epoch 8 Batch 100 Loss 0.1027\n",
            "Epoch 8 Loss 0.1122\n",
            "Time taken for 1 epoch 108.34366822242737 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1653\n",
            "Epoch 9 Batch 100 Loss 0.0950\n",
            "Epoch 9 Loss 0.1064\n",
            "Time taken for 1 epoch 108.30213785171509 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0782\n",
            "Epoch 10 Batch 100 Loss 0.0721\n",
            "Epoch 10 Loss 0.0811\n",
            "Time taken for 1 epoch 107.70187401771545 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0677\n",
            "Epoch 11 Batch 100 Loss 0.0835\n",
            "Epoch 11 Loss 0.0695\n",
            "Time taken for 1 epoch 107.26287078857422 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0571\n",
            "Epoch 12 Batch 100 Loss 0.0754\n",
            "Epoch 12 Loss 0.0676\n",
            "Time taken for 1 epoch 106.93268418312073 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0642\n",
            "Epoch 13 Batch 100 Loss 0.0614\n",
            "Epoch 13 Loss 0.0709\n",
            "Time taken for 1 epoch 107.53143358230591 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0440\n",
            "Epoch 14 Batch 100 Loss 0.0462\n",
            "Epoch 14 Loss 0.0505\n",
            "Time taken for 1 epoch 108.24450874328613 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0415\n",
            "Epoch 15 Batch 100 Loss 0.0453\n",
            "Epoch 15 Loss 0.0428\n",
            "Time taken for 1 epoch 107.48967361450195 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0313\n",
            "Epoch 16 Batch 100 Loss 0.0394\n",
            "Epoch 16 Loss 0.0386\n",
            "Time taken for 1 epoch 107.75594472885132 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0335\n",
            "Epoch 17 Batch 100 Loss 0.0691\n",
            "Epoch 17 Loss 0.0555\n",
            "Time taken for 1 epoch 107.33053731918335 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0497\n",
            "Epoch 18 Batch 100 Loss 0.0430\n",
            "Epoch 18 Loss 0.0437\n",
            "Time taken for 1 epoch 107.89942526817322 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0287\n",
            "Epoch 19 Batch 100 Loss 0.0372\n",
            "Epoch 19 Loss 0.0340\n",
            "Time taken for 1 epoch 107.32437682151794 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0220\n",
            "Epoch 20 Batch 100 Loss 0.0316\n",
            "Epoch 20 Loss 0.0292\n",
            "Time taken for 1 epoch 107.80276489257812 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0202\n",
            "Epoch 21 Batch 100 Loss 0.0420\n",
            "Epoch 21 Loss 0.0347\n",
            "Time taken for 1 epoch 107.11224174499512 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0514\n",
            "Epoch 22 Batch 100 Loss 0.0489\n",
            "Epoch 22 Loss 0.0409\n",
            "Time taken for 1 epoch 107.78998899459839 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0534\n",
            "Epoch 23 Batch 100 Loss 0.0364\n",
            "Epoch 23 Loss 0.0415\n",
            "Time taken for 1 epoch 106.71581983566284 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0242\n",
            "Epoch 24 Batch 100 Loss 0.0300\n",
            "Epoch 24 Loss 0.0292\n",
            "Time taken for 1 epoch 106.95878148078918 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0242\n",
            "Epoch 25 Batch 100 Loss 0.0271\n",
            "Epoch 25 Loss 0.0255\n",
            "Time taken for 1 epoch 107.18981528282166 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0177\n",
            "Epoch 26 Batch 100 Loss 0.0217\n",
            "Epoch 26 Loss 0.0225\n",
            "Time taken for 1 epoch 107.51780009269714 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0180\n",
            "Epoch 27 Batch 100 Loss 0.0743\n",
            "Epoch 27 Loss 0.0370\n",
            "Time taken for 1 epoch 107.64546322822571 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0305\n",
            "Epoch 28 Batch 100 Loss 0.0424\n",
            "Epoch 28 Loss 0.0375\n",
            "Time taken for 1 epoch 107.42950916290283 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0287\n",
            "Epoch 29 Batch 100 Loss 0.0267\n",
            "Epoch 29 Loss 0.0266\n",
            "Time taken for 1 epoch 107.13653063774109 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0198\n",
            "Epoch 30 Batch 100 Loss 0.0254\n",
            "Epoch 30 Loss 0.0211\n",
            "Time taken for 1 epoch 107.48024845123291 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S-Ay0ypoQMC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Evaluate function -- similar to the training loop\n",
        "def evaluate(sentence):\n",
        "\n",
        "  # Attention plot (to be plotted later on) -- initialized with max_lengths of both target and input\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  '''# Preprocess the sentence given\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # Fetch the indices concerning the words in the sentence and pad the sequence\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]'''\n",
        "  inputs = gt_rep(sentence,eng_alpha2index)\n",
        "\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  # Convert the inputs to tensors\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([0], 0)\n",
        "\n",
        "  # Loop until the max_length is reached for the target lang (ENGLISH)\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # Store the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    # Get the prediction with the maximum attention\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    # Append the token to the result\n",
        "    result += hindi_index2alpha[predicted_id]\n",
        "\n",
        "    # If <end> token is reached, return the result, input, and attention plot\n",
        "    if hindi_index2alpha[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # The predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvlwuuRg0Oj2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5shDzxao0aL5"
      },
      "source": [
        "# Translate function (which internally calls the evaluate function)\n",
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        " # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDXc2tJb1R5k",
        "outputId": "db544d79-014f-4be2-8654-e01c2ef37ae4"
      },
      "source": [
        "# Restore the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbf2b7c4ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWVl4ZaC1Usz",
        "outputId": "beb66caa-5e85-4870-a032-48d2aa3bfbf3"
      },
      "source": [
        "translate(\"krishna\")"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: krishna\n",
            "Predicted translation: कृष्ण<end>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}