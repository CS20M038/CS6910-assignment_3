{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled26.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "taqHKYQfAwJZ"
      },
      "source": [
        "#@title Import Libraries\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import models\n",
        "from numpy import array_equal\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from scipy.ndimage.interpolation import shift\n",
        "import csv\n",
        "import random"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L671_f3TZeG"
      },
      "source": [
        "input_dict=['-PAD-']\n",
        "target_dict=['-PAD-']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V2tIUKXApHB",
        "outputId": "3b24873c-1d52-4192-838f-6cdc0f60d366"
      },
      "source": [
        "eng_alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
        "pad_char = '-PAD-'\n",
        "\n",
        "eng_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "    eng_alpha2index[alpha] = index+1\n",
        "    input_dict.append(alpha)\n",
        "\n",
        "print(eng_alpha2index)\n",
        "print(input_dict)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "['-PAD-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0jaWOBrTvxG",
        "outputId": "799c9605-c62d-4609-9100-fcf9e3e0ed2d"
      },
      "source": [
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "    hindi_alpha2index[alpha] = index+1\n",
        "    target_dict.append(alpha)\n",
        "    #print(alpha)\n",
        "\n",
        "print(hindi_alpha2index)\n",
        "print(target_dict)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n",
            "['-PAD-', 'ऀ', 'ँ', 'ं', 'ः', 'ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', 'स', 'ह', 'ऺ', 'ऻ', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'ॎ', 'ॏ', 'ॐ', '॑', '॒', '॓', '॔', 'ॕ', 'ॖ', 'ॗ', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', 'ॠ', 'ॡ', 'ॢ', 'ॣ', '।', '॥', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '॰', 'ॱ', 'ॲ', 'ॳ', 'ॴ', 'ॵ', 'ॶ', 'ॷ', 'ॸ', 'ॹ', 'ॺ', 'ॻ', 'ॼ', 'ॽ', 'ॾ', 'ॿ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1b6DJtjA93_"
      },
      "source": [
        "import re\n",
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "# Remove all English non-letters\n",
        "def cleanEnglishVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ').upper()\n",
        "    line = non_eng_letters_regex.sub('', line)\n",
        "    return line.split()\n",
        "\n",
        "# Remove all Hindi non-letters\n",
        "def cleanHindiVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ')\n",
        "    cleaned_line = ''\n",
        "    for char in line:\n",
        "        if char in hindi_alpha2index or char == ' ':\n",
        "            cleaned_line += char\n",
        "    return cleaned_line.split()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9noN51CIBNXG"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "class TransliterationDataLoader(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.eng_words, self.hindi_words = self.readXmlDataset(filename, cleanHindiVocab)\n",
        "        self.shuffle_indices = list(range(len(self.eng_words)))\n",
        "        random.shuffle(self.shuffle_indices)\n",
        "        self.shuffle_start_index = 0\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.eng_words)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.eng_words[idx], self.hindi_words[idx]\n",
        "    \n",
        "    def readXmlDataset(self, filename, lang_vocab_cleaner):\n",
        "        tsv_file = open(filename)\n",
        "        read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
        "        lang1_words = []\n",
        "        lang2_words = []\n",
        "\n",
        "        for row in read_tsv:\n",
        "            lang2_words.append(row[0])\n",
        "            lang1_words.append(row[1])\n",
        "            #print(row[0])\n",
        "\n",
        "        return lang1_words, lang2_words\n",
        "    \n",
        "    def get_random_sample(self):\n",
        "        return self.__getitem__(np.random.randint(len(self.eng_words)))\n",
        "    \n",
        "    def get_batch_from_array(self, batch_size, array):\n",
        "        end = self.shuffle_start_index + batch_size\n",
        "        batch = []\n",
        "        if end >= len(self.eng_words):\n",
        "            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n",
        "            end = len(self.eng_words)\n",
        "        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index : end]]\n",
        "    \n",
        "    def get_batch(self, batch_size, postprocess = True):\n",
        "        eng_batch = self.get_batch_from_array(batch_size, self.eng_words)\n",
        "        hindi_batch = self.get_batch_from_array(batch_size, self.hindi_words)\n",
        "        self.shuffle_start_index += batch_size + 1\n",
        "        \n",
        "        # Reshuffle if 1 epoch is complete\n",
        "        if self.shuffle_start_index >= len(self.eng_words):\n",
        "            random.shuffle(self.shuffle_indices)\n",
        "            self.shuffle_start_index = 0\n",
        "            \n",
        "        return eng_batch, hindi_batch"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrT4bRcBPng"
      },
      "source": [
        "train_data = TransliterationDataLoader('hi.translit.sampled.train.tsv')\n",
        "test_data = TransliterationDataLoader('hi.translit.sampled.test.tsv')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEFtGYMuBfS8"
      },
      "source": [
        "def readXmlDataset(filename):\n",
        "        tsv_file = open(filename)\n",
        "        read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
        "        lang1_words = []\n",
        "        lang2_words = []\n",
        "\n",
        "        for row in read_tsv:\n",
        "            lang2_words.append(row[0])\n",
        "            lang1_words.append(row[1])\n",
        "            #print(row[0])\n",
        "\n",
        "        return lang1_words, lang2_words"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NBZMJ6JBkzm"
      },
      "source": [
        "input_texts, target_texts = readXmlDataset('hi.translit.sampled.train.tsv')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "johlcH8OBn5c",
        "outputId": "862bdb37-2084-441d-bce6-d2cb7a94e98e"
      },
      "source": [
        "for i, char in enumerate(\"एससी\"):\n",
        "    print(char)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ए\n",
            "स\n",
            "स\n",
            "ी\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM21kyxWQYNH",
        "outputId": "9004c5e1-8dd0-4bee-beec-5fbc3fda5d3b"
      },
      "source": [
        "def createDataset(datasets):\n",
        "    X=[]\n",
        "    for i, dataset in enumerate (datasets):\n",
        "        data=[]\n",
        "        for t, char in enumerate(dataset):\n",
        "            data.append(char)\n",
        "        X.append(data)\n",
        "    return X\n",
        "X=createDataset(input_texts)\n",
        "print(X[5555])\n",
        "X=createDataset(target_texts)\n",
        "print(X[5555])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'g', 'e', 'n', 'd', 'a']\n",
            "['ए', 'ज', 'े', 'ं', 'ड', 'ा']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkF5cRK2mpu-",
        "outputId": "694068de-9001-4947-f188-0d15eeb4c637"
      },
      "source": [
        "ax=X[5555]\n",
        "print(ax)\n",
        "bx=ax[0]+ax[1]+ax[2]+ax[3]+ax[4]+ax[5]\n",
        "print(b)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ए', 'ज', 'े', 'ं', 'ड', 'ा']\n",
            "एजेंडा\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotzkLpoSV2q"
      },
      "source": [
        "import torch\n",
        "def word_rep(word, letter2index, device = 'cpu'):\n",
        "    rep = np.zeros((len(word)+1,  len(letter2index)))\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        rep[letter_index][pos] = 1\n",
        "    pad_pos = letter2index[pad_char]\n",
        "    rep[letter_index+1][pad_pos] = 1\n",
        "    return rep\n",
        "\n",
        "def gt_rep(word, letter2index, device = 'cpu'):\n",
        "    gt_rep = np.zeros([len(word)+1, 1], dtype=np.long)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        gt_rep[letter_index][0] = pos\n",
        "    gt_rep[letter_index+1][0] = letter2index[pad_char]\n",
        "    return gt_rep\n",
        "    word_rep('abc',eng_alpha2index).shape\n",
        "    x=gt_rep('abc',eng_alpha2index)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GSzR0daq-wp",
        "outputId": "3eacc5f4-9ac5-4b52-d09e-c4907811dc65"
      },
      "source": [
        "len(eng_alpha2index)\n",
        "a=[0]*26\n",
        "a.append(1)\n",
        "a"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykb34awGjEhm",
        "outputId": "90de42d2-e16c-4dc1-9e12-da7ab6685749"
      },
      "source": [
        "X_train=[]\n",
        "for i,data in enumerate(input_texts):\n",
        "    #print(data)\n",
        "    X_train.append(word_rep(data,eng_alpha2index))\n",
        "X_train = np.array(X_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnTp_qbksCcm",
        "outputId": "4119b6b5-9d8e-4913-e10c-8f2c853f30e7"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDSl1aTGo1Yj",
        "outputId": "8e388dbb-2e2a-4594-dc9d-d88b384260c4"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X_train_padded = pad_sequences(X_train, maxlen= 256, padding='post', value=a)\n",
        "print(\"X_train shape: \",X_train.shape)\n",
        "print(\"X_train_padded shape: \",X_train_padded.shape)\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (44204,)\n",
            "X_train_padded shape:  (44204, 256, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_4b6FWwtMDS"
      },
      "source": [
        "b=[0]*128\n",
        "b.append(1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwq8FfcFo4qi",
        "outputId": "8feb0915-4aa2-4a90-800a-0faa836103fc"
      },
      "source": [
        "y_train=[]\n",
        "for i,data in enumerate(target_texts):\n",
        "    #print(data)\n",
        "    y_train.append(word_rep(data,hindi_alpha2index))\n",
        "y_train = np.array(y_train)\n",
        "y_train[0]\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "y_train_padded = pad_sequences(y_train, maxlen= 256, padding='post', value=b)\n",
        "print(\"Y_train shape: \",y_train.shape)\n",
        "print(\"Y_train_padded shape: \",y_train_padded.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Y_train shape:  (44204,)\n",
            "Y_train_padded shape:  (44204, 256, 129)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LBr_H-81O5U",
        "outputId": "6bba75f3-14e6-44af-caa8-e52fa2b1b307"
      },
      "source": [
        "y_train_padded[5]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG7m8uHz09wD"
      },
      "source": [
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui3bWcoH2dbC",
        "outputId": "29bc68a1-0b0e-4351-bcd5-a39f3f1bdc17"
      },
      "source": [
        "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_index2alpha = {0: pad_char}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "    hindi_index2alpha[index+1] = alpha\n",
        "   # target_dict.append(alpha)\n",
        "    #print(alpha)\n",
        "\n",
        "print(hindi_index2alpha)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '-PAD-', 1: 'ऀ', 2: 'ँ', 3: 'ं', 4: 'ः', 5: 'ऄ', 6: 'अ', 7: 'आ', 8: 'इ', 9: 'ई', 10: 'उ', 11: 'ऊ', 12: 'ऋ', 13: 'ऌ', 14: 'ऍ', 15: 'ऎ', 16: 'ए', 17: 'ऐ', 18: 'ऑ', 19: 'ऒ', 20: 'ओ', 21: 'औ', 22: 'क', 23: 'ख', 24: 'ग', 25: 'घ', 26: 'ङ', 27: 'च', 28: 'छ', 29: 'ज', 30: 'झ', 31: 'ञ', 32: 'ट', 33: 'ठ', 34: 'ड', 35: 'ढ', 36: 'ण', 37: 'त', 38: 'थ', 39: 'द', 40: 'ध', 41: 'न', 42: 'ऩ', 43: 'प', 44: 'फ', 45: 'ब', 46: 'भ', 47: 'म', 48: 'य', 49: 'र', 50: 'ऱ', 51: 'ल', 52: 'ळ', 53: 'ऴ', 54: 'व', 55: 'श', 56: 'ष', 57: 'स', 58: 'ह', 59: 'ऺ', 60: 'ऻ', 61: '़', 62: 'ऽ', 63: 'ा', 64: 'ि', 65: 'ी', 66: 'ु', 67: 'ू', 68: 'ृ', 69: 'ॄ', 70: 'ॅ', 71: 'ॆ', 72: 'े', 73: 'ै', 74: 'ॉ', 75: 'ॊ', 76: 'ो', 77: 'ौ', 78: '्', 79: 'ॎ', 80: 'ॏ', 81: 'ॐ', 82: '॑', 83: '॒', 84: '॓', 85: '॔', 86: 'ॕ', 87: 'ॖ', 88: 'ॗ', 89: 'क़', 90: 'ख़', 91: 'ग़', 92: 'ज़', 93: 'ड़', 94: 'ढ़', 95: 'फ़', 96: 'य़', 97: 'ॠ', 98: 'ॡ', 99: 'ॢ', 100: 'ॣ', 101: '।', 102: '॥', 103: '०', 104: '१', 105: '२', 106: '३', 107: '४', 108: '५', 109: '६', 110: '७', 111: '८', 112: '९', 113: '॰', 114: 'ॱ', 115: 'ॲ', 116: 'ॳ', 117: 'ॴ', 118: 'ॵ', 119: 'ॶ', 120: 'ॷ', 121: 'ॸ', 122: 'ॹ', 123: 'ॺ', 124: 'ॻ', 125: 'ॼ', 126: 'ॽ', 127: 'ॾ', 128: 'ॿ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JxU4VTi51A_C",
        "outputId": "899f6022-0224-46eb-e774-865ff34346d1"
      },
      "source": [
        "d=one_hot_decode(y_train_padded[5])\n",
        "c=hindi_index2alpha[6]+hindi_index2alpha[3]+hindi_index2alpha[22]+hindi_index2alpha[66]+hindi_index2alpha[49]+hindi_index2alpha[64]+hindi_index2alpha[37]+hindi_index2alpha[0]+hindi_index2alpha[128]\n",
        "c\n",
        "c"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'अंकुरित-PAD-ॿ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "dogsnms-2Q-S",
        "outputId": "7fb325a9-210b-4393-d35a-48664fa9df5f"
      },
      "source": [
        "hindi_alpha2index[6]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-636acde006ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhindi_alpha2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 6"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KyXw5xnht50"
      },
      "source": [
        "\n",
        "#Prepare TRAIN data set\n",
        "encoder_input_data = X_train_padded.copy()\n",
        "decoder_target_data = y_train_padded.copy()\n",
        "decoder_input_data = decoder_target_data.copy()\n",
        "for i, samples in enumerate(decoder_target_data):\n",
        "  seq = one_hot_decode(samples)\n",
        "  shifted= shift(seq, 1, cval=0)\n",
        "  decoder_input_data[i]=one_hot_encode(shifted,input_dimension)\n",
        "print(\"Data for Train\")\n",
        "print('encoder_input_data (X): ', one_hot_decode(encoder_input_data[1]))\n",
        "print('decoder_input_data (teacher forcing): ',one_hot_decode(decoder_input_data[1]))\n",
        "print('decoder_target_data (y):',one_hot_decode(decoder_target_data[1]))\n",
        "print(encoder_input_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLSOKsoNQgwC",
        "outputId": "897b40a0-cacd-489b-8212-398a6aa01ad7"
      },
      "source": [
        "from numpy import argmax\n",
        "# define input string\n",
        "data = 'axz'\n",
        "print(data)\n",
        "# define universe of possible input values\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
        "# define a mapping of chars to integers\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[char] for char in data]\n",
        "print(integer_encoded)\n",
        "# one hot encode\n",
        "onehot_encoded = list()\n",
        "for value in integer_encoded:\n",
        "\tletter = [0 for _ in range(len(alphabet))]\n",
        "\tletter[value] = 1\n",
        "\tonehot_encoded.append(letter)\n",
        "print(onehot_encoded)\n",
        "# invert encoding\n",
        "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
        "print(inverted)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "axz\n",
            "[0, 23, 25]\n",
            "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n",
            "a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBbDPXJmQmKR"
      },
      "source": [
        "def one_hot_decode(encoded_seq):\n",
        "    a=[]\n",
        "    a=list([argmax(vector) for vector in onehot_encoded])\n",
        "    return a"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7LdDoqnaeKU",
        "outputId": "84a50376-b0ad-4ece-cb96-dc7ba3a2becb"
      },
      "source": [
        "arr = print(one_hot_decode(onehot_encoded))\n",
        "type(arr)\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 23, 25]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S5bEEADN1zW"
      },
      "source": [
        "#@title Functions to generate Seq2Seq Dataset, one hot encode / decode Input & Output Sequences\n",
        "\n",
        "\n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(input_texts, target_texts, input_vocab_size, target_vocab_size):\n",
        "    X=[[]]\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        data=[]\n",
        "        for t, char in enumerate(input_text):\n",
        "            data.append(char)\n",
        "         X.append(data)\n",
        "        break\n",
        "\treturn [randint(1, n_unique-1) for _ in \n",
        "\t        range( randint(min_timesteps_in, max_timesteps_in))]\n",
        "\n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        "\n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "# prepare data for the LSTM\n",
        "def get_reversed_pairs(min_timesteps_in, max_timesteps_in,vocabulary_size,verbose= False):\n",
        "\t# generate random sequence\n",
        "  a=[]\n",
        "  sequence_in = generate_sequence(min_timesteps_in, max_timesteps_in, vocabulary_size)\n",
        "  sequence_out = sequence_in[::-1]\n",
        "  \n",
        "  sequence_out =[i for i in sequence_out if i%2 ==0 and i != 0]\n",
        "\n",
        "  for i, char in enumerate(\"एससी\"):\n",
        "    a.append(char)  \n",
        "  print(type(sequence_in))\n",
        "  # one hot encode\n",
        "  X = one_hot_encode(sequence_in, vocabulary_size)\n",
        "    \n",
        "  y = one_hot_encode(sequence_out, vocabulary_size)\n",
        "  print(a)\n",
        "  return X,y\n",
        "\n",
        "\n",
        "def create_dataset(train_size, test_size, \n",
        "                   min_timesteps_in, max_timesteps_in, vocabulary_size, verbose= False):\n",
        "\tpairs = [get_reversed_pairs(min_timesteps_in, max_timesteps_in,vocabulary_size) for _ in range(train_size)]\n",
        "\t\n",
        "\tpairs=np.array(pairs).squeeze()\n",
        "\tX_train = pairs[:,0]\n",
        "\ty_train = pairs[:,1]\n",
        "\tpairs = [get_reversed_pairs(min_timesteps_in, max_timesteps_in,vocabulary_size) for _ in range(test_size)]\n",
        "\tpairs=np.array(pairs).squeeze()\n",
        "\tX_test = pairs[:,0]\n",
        "\ty_test = pairs[:,1]\t\n",
        "\n",
        "\tif(verbose):\n",
        "\t\tprint('\\nSample X and y sequences (in Raw Format)')\n",
        "\t\tfor i in range(5):\n",
        "\t\t\tprint('X[',i,']=%s' % (one_hot_decode(X_train[i])), '........ y[',i,']=%s' % (one_hot_decode(y_train[i])))\n",
        "\n",
        "\t\tprint('\\nEach input and output sequences are converted one_hot_encoded format with input_dimension = ',\n",
        "\t\t      vocabulary_size)\n",
        "\t\tprint('X[0]=\\n%s' % (X_train[0]))\n",
        "\t\tprint('y[0]=\\n%s' % (y_train[0]))\n",
        "\t\n",
        "\t\tprint('\\nGenerated sequence datasets as follows [sample_size,time_steps, input_dimension]')\n",
        "\t\tprint('X_train.shape: ', X_train.shape,'y_train.shape: ', y_train.shape)\n",
        "\t\tprint('X_test.shape: ', X_test.shape,'y_test.shape: ', y_test.shape)\n",
        "\t\n",
        "\treturn X_train, y_train, X_test, \ty_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVFH3gAhCP5Y",
        "outputId": "eb762506-a3ed-4d61-f776-5c446808c389"
      },
      "source": [
        "print(target_texts[5999])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "एससी\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}